{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "- We use three Keras functionalities (inherits from keras model) to train, evaluate and pedict from the model that developed\n",
    "    - Model.fit() - for training\n",
    "    - Model.evaluate() - to evaluate the model\n",
    "    - Model.predict() - to infere the model on test example or test dataset\n",
    "- When we use built in loops for training and evaluation, process will be same for both Seqential and Functional API models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A First end -to-end Example\n",
    "- Data can be fed to training loops either using\n",
    "    - Numpy Arrays (When the data is small and can be fit into memory)\n",
    "    - tf.data Dataset objects\n",
    "\n",
    "Lets consider the following model for MNIST classification:\n",
    "\n",
    "A Typical end-to-end workflow looks like consists of:\n",
    "- Training\n",
    "- Validation on Hold out Data (generated from original training data)\n",
    "- Evaluation on Test Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
    "x = layers.Dense(64, activation=\"relu\", name=\"dense1\")(inputs)\n",
    "x = layers.Dense(64, activation=\"relu\", name=\"dense2\")(x)\n",
    "outputs = layers.Dense(10, activation=\"softmax\", name=\"predictio\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training The Model...\n",
      "Epoch 1/2\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.3406 - sparse_categorical_accuracy: 0.9034 - val_loss: 0.2176 - val_sparse_categorical_accuracy: 0.9380\n",
      "Epoch 2/2\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.1671 - sparse_categorical_accuracy: 0.9503 - val_loss: 0.1396 - val_sparse_categorical_accuracy: 0.9600\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Preprocess the data\n",
    "x_train = x_train.reshape(-1,784).astype(\"float32\")/255\n",
    "x_test = x_test.reshape(-1,784).astype(\"float32\")/255\n",
    "\n",
    "y_train = y_train.astype(\"float32\")\n",
    "y_test = y_test.astype(\"float32\")\n",
    "\n",
    "# Reserve 10000 samples for validation\n",
    "x_val = x_train[-10000:]\n",
    "y_val = y_train[-10000:]\n",
    "x_train = x_train[:-10000]\n",
    "y_train = y_train[:-10000]\n",
    "\n",
    "# Specify the training configuration (optimizser, loss, metrics)\n",
    "model.compile(optimizer = keras.optimizers.RMSprop(),\n",
    "              loss = keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics = [keras.metrics.SparseCategoricalAccuracy()],)\n",
    "\n",
    "# fit() is for training the model with several parameters\n",
    "print(\"Training The Model...\")\n",
    "history = model.fit(x_train, y_train, batch_size=64, epochs=2, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.34058651328086853, 0.16712217032909393],\n",
       " 'sparse_categorical_accuracy': [0.9034000039100647, 0.9503399729728699],\n",
       " 'val_loss': [0.21763567626476288, 0.13958387076854706],\n",
       " 'val_sparse_categorical_accuracy': [0.9380000233650208, 0.9599999785423279]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# history object holds the metrics and losses for each epoch of both training and validation data\n",
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evauae on Test Data\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.1391 - sparse_categorical_accuracy: 0.9585\n",
      "test loss, test acc: [0.13907140493392944, 0.9585000276565552]\n",
      "Generate Predictions for 3 samples\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "predictions shape (3, 10)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data using \"evaluate\" method\n",
    "print(\"Evauae on Test Data\")\n",
    "results = model.evaluate(x_test, y_test, batch_size=128)\n",
    "print(\"test loss, test acc:\", results)\n",
    "# Get predictions on individual images or batch of images using predict method\n",
    "print(\"Generate Predictions for 3 samples\")\n",
    "predictions = model.predict(x_test[:3])\n",
    "print(\"predictions shape\", predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling a Model: Loss, Metrics, Optimizer\n",
    "To train a model before going to fit() we need to compile the model with following fields\n",
    "- optimizer - Algorithm for Backpropogation (example: Adam, RMSProp, Adagrad,...etc)\n",
    "- loss - If the model have multiple outputs then we can specify different loss functions for each output\n",
    "- metrics - its list of where we can specify any number of metrics. and also for multi output model we can specify multiple types of metrics\n",
    "\n",
    "If we want to go with default values for (optimizer, loss and metrics) we can specify them in strings. if we want to customize them we need to call respective functions from keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model compilation with default fileds\n",
    "model.compile(\n",
    "    optimizer=\"rmsprop\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"sparse_categorical_accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model compilation with custom functions\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For later reuse, let's put our model definition and compile step in functions; we will call them several times across different examples in this guide.\n",
    "def get_uncompiled_model():\n",
    "    inputs = keras.Input(shape=(784,), name=\"digits\")\n",
    "    x = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs)\n",
    "    x = layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x)\n",
    "    outputs = layers.Dense(10, activation=\"softmax\", name=\"predictions\")(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_compiled_model():\n",
    "    model = get_uncompiled_model()\n",
    "    model.compile(\n",
    "        optimizer=\"rmsprop\",\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"sparse_categorical_accuracy\"],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Builtin Optimizers, Losses and Metircs\n",
    "Optimizers: tf.keras.optimizers.\n",
    "- SGD() (with or without momentum)\n",
    "- RMSProp()\n",
    "- Adam()\n",
    "- Adagard()\n",
    "- Adadelta()\n",
    "- Adamax()\n",
    "\n",
    "Losses: tf.keras.losses.\n",
    "- BinaryCrossentropy()\n",
    "- CategoricalCrossentropy()\n",
    "- CategoricalHinge()\n",
    "- CosineSimilairty()\n",
    "- Hinge()\n",
    "- KLDivergence()\n",
    "- MeanAbsoluteError()\n",
    "- MeanAbsolutePercentageError()\n",
    "- MeanSquaredError()\n",
    "- MeanSquaredLogarithmicError()\n",
    "- SparseCategoricalCrossentropy()\n",
    "\n",
    "Metrics: tf.keras.metrics.\n",
    "- AUC()\n",
    "- Precision()\n",
    "- Recall()\n",
    "- Accuracy()\n",
    "\n",
    "Apart from there if we want to create a custom functions Keras has the feasability to create\n",
    "\n",
    "### Custom Loss:\n",
    "To create a custom loss function we can do it in two ways\n",
    "- create a function which takes y_true and y_pred as inputs (it wont accept other inputs)\n",
    "- if we want to have other paramters along with y_true and y_pred we need to create a custom loss class inherited from tf.keras.losses.Loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0161\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2716fc22af0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The following example shows a loss function that computes the mean squared error between the real data and the predictions:\n",
    "def custom_mean_squared_error(y_true, y_pred):\n",
    "    return tf.math.reduce_mean(tf.square(y_true - y_pred))\n",
    "\n",
    "\n",
    "model = get_uncompiled_model()\n",
    "model.compile(optimizer=keras.optimizers.Adam(), loss=custom_mean_squared_error)\n",
    "\n",
    "# We need to one-hot encode the labels to use MSE\n",
    "y_train_one_hot = tf.one_hot(y_train, depth=10)\n",
    "model.fit(x_train, y_train_one_hot, batch_size=64, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need a loss function that takes in parameters beside y_true and y_pred, you can subclass the tf.keras.losses.Loss class and implement the following two methods:\n",
    "- __init__(self): accept parameters to pass during the call of your loss function\n",
    "- call(self, y_true, y_pred): use the targets (y_true) and the model predictions (y_pred) to compute the model's loss\n",
    "\n",
    "Let's say you want to use mean squared error, but with an added term that will de-incentivize prediction values far from 0.5 (we assume that the categorical targets are one-hot encoded and take values between 0 and 1). This creates an incentive for the model not to be too confident, which may help reduce overfitting (we won't know if it works until we try!).\n",
    "\n",
    "Here's how you would do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0387\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2716fa31520>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CustomMSE(tf.keras.losses.Loss):\n",
    "    def __init__(self, regularization_factor=0.1, name=\"custom_mse\"):\n",
    "        super(CustomMSE, self).__init__(name=name)\n",
    "        self.regularization_factor = regularization_factor\n",
    "    def call(self, y_true, y_pred):\n",
    "        mse = tf.reduce_mean(tf.square(y_true-y_pred))\n",
    "        reg = tf.math.reduce_mean(tf.square(0.5 - y_pred))\n",
    "        return mse+reg*self.regularization_factor\n",
    "\n",
    "model = get_uncompiled_model()\n",
    "model.compile(optimizer=keras.optimizers.Adam(), loss=CustomMSE())\n",
    "\n",
    "y_train_one_hot = tf.one_hot(y_train, depth=10)\n",
    "model.fit(x_train, y_train_one_hot, batch_size=64, epochs=1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Metrics:\n",
    "if we want to create or use any metric which is not a part of API we can create it by using tf.keras.metrics.Metric class. For this we need to implement 4 methods:\n",
    "- __init__(self) in which we create state variables for our metric\n",
    "- update_state(self, y_true, y_pred, sample_weight=None) which takes y_true and y_pred to update the sate varaibles\n",
    "- result(self) which uses the state varaible to compute the result\n",
    "- reset_state(Self) which reinitializes the state of the metric\n",
    "\n",
    "State update and result computations kept separately because when the data size used for the results computation is vere huge, it will become computationally very expensive and would be done periodically. So for each period state varaibles will be updated and corresponing results as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.3406 - categorical_true_positives: 45142.0000\n",
      "Epoch 2/3\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.1581 - categorical_true_positives: 47628.0000\n",
      "Epoch 3/3\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.1141 - categorical_true_positives: 48309.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27172190e20>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here's a simple example showing how to implement a CategoricalTruePositives metric that counts how many samples were correctly classified as belonging to a given class:\n",
    "class CategoricalTruePositives(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name=\"categorical_true_positives\", **kwargs):\n",
    "        super(CategoricalTruePositives, self).__init__(name=name, **kwargs)\n",
    "        self.true_positives = self.add_weight(name=\"ctp\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = tf.reshape(tf.argmax(y_pred, axis=1), (-1,1))\n",
    "        values = tf.cast(y_true, \"int32\") == tf.cast(y_pred, \"int32\")\n",
    "        values = tf.cast(values, \"float32\")\n",
    "        if sample_weight is not None:\n",
    "            sample_weight = tf.cast(sample_weight, \"float32\")\n",
    "            values = tf.multiply(values, sample_weight)\n",
    "        self.true_positives.assign_add(tf.reduce_sum(values))\n",
    "\n",
    "    def result(self):\n",
    "        return self.true_positives\n",
    "    \n",
    "    def reset_state(self):\n",
    "        # The state of the metric will be reset at the start of each epoch.\n",
    "        self.true_positives.assign(0.0)\n",
    "\n",
    "model = get_uncompiled_model()\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=[CategoricalTruePositives()],\n",
    ")\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Losses Metrics that don' fit the standard signature\n",
    "- Many of losses and metrics can be computed using y_true and y_pred\n",
    "- But in some cases model output might not be used to compute the loss\n",
    "- For example a regularization loss may only require the activation of a layer, and this activation may not be a model output.\n",
    "- In such cases, you can call self.add_loss(loss_value) from inside the call method of a custom layer. \n",
    "- Losses added in this way get added to the \"main\" loss during training (the one passed to compile())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 2s 2ms/step - loss: 2.4952\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2717224dd00>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ActivityRegularizationLayer(tf.keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        self.add_loss(tf.reduce_sum(inputs)*0.1)\n",
    "        return inputs\n",
    "\n",
    "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
    "x = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs)\n",
    "\n",
    "# Insert activity regularization as a layer\n",
    "x = ActivityRegularizationLayer()(x)\n",
    "x = layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x)\n",
    "outputs = layers.Dense(10, name=\"predictions\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    ")\n",
    "\n",
    "# The displayed loss will be much higher than before\n",
    "# due to the regularization component.\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 2s 2ms/step - loss: 0.3438 - std_of_activation: 0.9974\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x271726b64f0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can do the same for logging metric values, using add_metric()\n",
    "class MetricLoggingLayer(tf.keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        # The `aggregation` argument defines\n",
    "        # how to aggregate the per-batch values\n",
    "        # over each epoch:\n",
    "        # in this case we simply average them.\n",
    "        self.add_metric(\n",
    "            tf.keras.backend.std(inputs),name=\"std_of_activation\", aggregation=\"mean\"\n",
    "        )\n",
    "        return inputs\n",
    "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
    "x = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs)\n",
    "\n",
    "# Insert std logging as a layer.\n",
    "x = MetricLoggingLayer()(x)\n",
    "\n",
    "x = layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x)\n",
    "outputs = layers.Dense(10, name=\"predictions\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    ")\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 2s 2ms/step - loss: 2.4869 - std_of_activation: 0.0019\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x271727f4130>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In the Functional API, you can also call model.add_loss(loss_tensor), or model.add_metric(metric_tensor, name, aggregation).\n",
    "# Here's a simple example:\n",
    "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
    "x1 = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs)\n",
    "x2 = layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x1)\n",
    "outputs = layers.Dense(10, name=\"predictions\")(x2)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.add_loss(tf.reduce_sum(x1) * 0.1)\n",
    "\n",
    "model.add_metric(keras.backend.std(x1), name=\"std_of_activation\", aggregation=\"mean\")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    ")\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when you pass losses via add_loss(), it becomes possible to call compile() without a loss function, since the model already has a loss to minimize.\n",
    "\n",
    "Consider the following LogisticEndpoint layer: it takes as inputs targets & logits, and it tracks a crossentropy loss via add_loss(). It also tracks classification accuracy via add_metric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 596ms/step - loss: 0.9438 - binary_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2717295b490>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LogisticEndpoint(tf.keras.layers.Layer):\n",
    "    def __init__(self, name=None):\n",
    "        super(LogisticEndpoint, self).__init__(name=name)\n",
    "        self.loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "        self.accuracy_fn = tf.keras.metrics.BinaryAccuracy()\n",
    "\n",
    "    def call(self, targets, logits, sample_weights=None):\n",
    "        # Compute the training-time loss value and add it\n",
    "        # to the layer using `self.add_loss()`.\n",
    "        loss = self.loss_fn(targets, logits, sample_weights)\n",
    "        self.add_loss(loss)\n",
    "        # Log accuracy as a metric and add it\n",
    "        # to the layer using `self.add_metric()`.\n",
    "        acc = self.accuracy_fn(targets, logits, sample_weights)\n",
    "        self.add_metric(acc, name=\"accuracy\")\n",
    "        # Return the inference-time prediction tensor (for `.predict()`).\n",
    "        return tf.nn.softmax(logits)\n",
    "\n",
    "#you can use it in a model with two inputs (input data & targets), compiled without a loss argument, like this:\n",
    "inputs = keras.Input(shape=(3,), name=\"inputs\")\n",
    "targets = keras.Input(shape=(10,), name=\"targets\")\n",
    "logits = keras.layers.Dense(10)(inputs)\n",
    "predictions = LogisticEndpoint(name=\"predictions\")(logits, targets)\n",
    "\n",
    "model = keras.Model(inputs=[inputs, targets], outputs=predictions)\n",
    "model.compile(optimizer=\"adam\")  # No loss argument!\n",
    "\n",
    "data = {\n",
    "    \"inputs\": np.random.random((3, 3)),\n",
    "    \"targets\": np.random.random((3, 10)),\n",
    "}\n",
    "model.fit(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatically Setting Apart a Validation Holdout Set\n",
    "- So far we have used validation data set for validating the model while training\n",
    "- Instead we can split the training data into vaidation while training using validation_split paramters\n",
    "- But this can be used only when we are using Numpy data for training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 2s 3ms/step - loss: 0.3744 - sparse_categorical_accuracy: 0.8953 - val_loss: 0.2447 - val_sparse_categorical_accuracy: 0.9270\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27172a0dca0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_compiled_model()\n",
    "model.fit(x_train, y_train, batch_size=64, validation_split=0.2, epochs=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation from tf.data Datasets\n",
    "- So far we worked with Numpy array datasets\n",
    "- Now lets look at the case where our data comes in the form of tf.data.Dataset object.\n",
    "- The tf.data API is a set of utilities in Tensorflow 2.0 for loading and preprocessing data in a way that's fast and scalable\n",
    "- We cah pass a Dataset instance directly to the methods fit(), evaluate() and predict():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.3379 - sparse_categorical_accuracy: 0.9052\n",
      "Epoch 2/3\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.1609 - sparse_categorical_accuracy: 0.9525\n",
      "Epoch 3/3\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.1167 - sparse_categorical_accuracy: 0.9657\n",
      "Evaluate...\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.1253 - sparse_categorical_accuracy: 0.9616\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 0.12527132034301758,\n",
       " 'sparse_categorical_accuracy': 0.9616000056266785}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_compiled_model()\n",
    "# Lets create a Datset instance on MNIST data \n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "# shuffle and slice the datset\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "# Now do the same for test dataset\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_dataset = test_dataset.batch(64)\n",
    "\n",
    "# since we passed batching in dataset there is no need of providing batch_size in training\n",
    "model.fit(train_dataset, epochs=3)\n",
    "\n",
    "# lets evaluate on test dataset\n",
    "print(\"Evaluate...\")\n",
    "result = model.evaluate(test_dataset)\n",
    "dict(zip(model.metrics_names,result))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dataset will be reset at the end of the each epoch, so that it can be reused in the next epoch. \n",
    "- If we want to run only a specific numver of batches for each epoch we can pass the argument steps_per_epoch\n",
    "- This argument specifies howmany training stpes the model should run using the Dataset before moving on to the next epoch. \n",
    "- If we do this the Dataset is not reset at the end of the each epoch, insted we just keep drawing the next batches. \n",
    "- The dataset will eventually run outof data (unless it is an infinitely-looping dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/11\n",
      "100/100 [==============================] - 1s 2ms/step - loss: 0.8155 - sparse_categorical_accuracy: 0.7886\n",
      "Epoch 2/11\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.3797 - sparse_categorical_accuracy: 0.8867\n",
      "Epoch 3/11\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.3248 - sparse_categorical_accuracy: 0.9087\n",
      "Epoch 4/11\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.2743 - sparse_categorical_accuracy: 0.9205\n",
      "Epoch 5/11\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.2737 - sparse_categorical_accuracy: 0.9230\n",
      "Epoch 6/11\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.2322 - sparse_categorical_accuracy: 0.9334\n",
      "Epoch 7/11\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.2287 - sparse_categorical_accuracy: 0.9336\n",
      "Epoch 8/11\n",
      " 67/100 [===================>..........] - ETA: 0s - loss: 0.2239 - sparse_categorical_accuracy: 0.9312WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 1100 batches). You may need to use the repeat() function when building your dataset.\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.2219 - sparse_categorical_accuracy: 0.9331\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2710b4697f0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_compiled_model()\n",
    "# prepare the training dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "# only use 1000 vatches per epoch (thats 64*100 samples)\n",
    "model.fit(train_dataset, epochs=11, steps_per_epoch = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/11\n",
      "100/100 [==============================] - 1s 3ms/step - loss: 0.8181 - sparse_categorical_accuracy: 0.7916\n",
      "Epoch 2/11\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.3900 - sparse_categorical_accuracy: 0.8841\n",
      "Epoch 3/11\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.3286 - sparse_categorical_accuracy: 0.9039\n",
      "Epoch 4/11\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.2615 - sparse_categorical_accuracy: 0.9228\n",
      "Epoch 5/11\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.2761 - sparse_categorical_accuracy: 0.9195\n",
      "Epoch 6/11\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.2296 - sparse_categorical_accuracy: 0.9281\n",
      "Epoch 7/11\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.2312 - sparse_categorical_accuracy: 0.9297\n",
      "Epoch 8/11\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.2234 - sparse_categorical_accuracy: 0.9341\n",
      "Epoch 9/11\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.1715 - sparse_categorical_accuracy: 0.9505\n",
      "Epoch 10/11\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.1874 - sparse_categorical_accuracy: 0.9467\n",
      "Epoch 11/11\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.1628 - sparse_categorical_accuracy: 0.9522\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2710c7a7130>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_compiled_model()\n",
    "# prepare the training dataset using repeat function\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).repeat()\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "# only use 1000 vatches per epoch (thats 64*100 samples)\n",
    "model.fit(train_dataset, epochs=11, steps_per_epoch = 100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a Validation Dataset\n",
    "- We can pass a Dataset instance as a validation_data argument in fit():\n",
    "- At the end of each epoch the model will iterate over the validation dataset and compute the validation loss and validation metrics\n",
    "- If we want to run validation only on a specific number of batches from this dataset, we can pass the validation_steps argument\n",
    "- this argument specifies how many validation steps the model should run with the validation dataset before interrupting validation and moving on to the next epoch.\n",
    "- Note that the validation dataset will be reset after each use even with validation_steps argument (so that you will always be evaluating on the same samples from epoch to epoch).\n",
    "- The argument validation_split (generating a holdout set from the training data) is not supported when training from Dataset objects, since this feature requires the ability to index the samples of the datasets, which is not possible in general with the Dataset API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 3s 3ms/step - loss: 0.3448 - sparse_categorical_accuracy: 0.9018 - val_loss: 0.1838 - val_sparse_categorical_accuracy: 0.9466\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2710c9aa8b0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_compiled_model()\n",
    "\n",
    "# Prepare the training dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "# Prepare the validation dataset\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_dataset = val_dataset.batch(64)\n",
    "\n",
    "model.fit(train_dataset, epochs=1, validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 3s 2ms/step - loss: 0.3285 - sparse_categorical_accuracy: 0.9083 - val_loss: 0.3002 - val_sparse_categorical_accuracy: 0.9297\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2710c9aa610>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_compiled_model()\n",
    "\n",
    "# Prepare the training dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "# Prepare the validation dataset\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_dataset = val_dataset.batch(64)\n",
    "\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    epochs=1,\n",
    "    # Only run validation using the first 10 batches of the dataset\n",
    "    # using the `validation_steps` argument\n",
    "    validation_data=val_dataset,\n",
    "    validation_steps=10,\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Input Formats Supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "127b1e9fcfedd5dda8ac936388b1f24776168c431ca343c8354d3bc75daf36ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
