{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "- We use three Keras functionalities (inherits from keras model) to train, evaluate and pedict from the model that developed\n",
    "    - Model.fit() - for training\n",
    "    - Model.evaluate() - to evaluate the model\n",
    "    - Model.predict() - to infere the model on test example or test dataset\n",
    "- When we use built in loops for training and evaluation, process will be same for both Seqential and Functional API models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A First end -to-end Example\n",
    "- Data can be fed to training loops either using\n",
    "    - Numpy Arrays (When the data is small and can be fit into memory)\n",
    "    - tf.data Dataset objects\n",
    "\n",
    "Lets consider the following model for MNIST classification:\n",
    "\n",
    "A Typical end-to-end workflow looks like consists of:\n",
    "- Training\n",
    "- Validation on Hold out Data (generated from original training data)\n",
    "- Evaluation on Test Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
    "x = layers.Dense(64, activation=\"relu\", name=\"dense1\")(inputs)\n",
    "x = layers.Dense(64, activation=\"relu\", name=\"dense2\")(x)\n",
    "outputs = layers.Dense(10, activation=\"softmax\", name=\"predictio\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training The Model...\n",
      "Epoch 1/2\n",
      "782/782 [==============================] - 7s 7ms/step - loss: 0.3458 - sparse_categorical_accuracy: 0.9023 - val_loss: 0.1952 - val_sparse_categorical_accuracy: 0.9451\n",
      "Epoch 2/2\n",
      "782/782 [==============================] - 5s 7ms/step - loss: 0.1646 - sparse_categorical_accuracy: 0.9519 - val_loss: 0.1422 - val_sparse_categorical_accuracy: 0.9587\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Preprocess the data\n",
    "x_train = x_train.reshape(-1,784).astype(\"float32\")/255\n",
    "x_test = x_test.reshape(-1,784).astype(\"float32\")/255\n",
    "\n",
    "y_train = y_train.astype(\"float32\")\n",
    "y_test = y_test.astype(\"float32\")\n",
    "\n",
    "# Reserve 10000 samples for validation\n",
    "x_val = x_train[-10000:]\n",
    "y_val = y_train[-10000:]\n",
    "x_train = x_train[:-10000]\n",
    "y_train = y_train[:-10000]\n",
    "\n",
    "# Specify the training configuration (optimizser, loss, metrics)\n",
    "model.compile(optimizer = keras.optimizers.RMSprop(),\n",
    "              loss = keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics = [keras.metrics.SparseCategoricalAccuracy()],)\n",
    "\n",
    "# fit() is for training the model with several parameters\n",
    "print(\"Training The Model...\")\n",
    "history = model.fit(x_train, y_train, batch_size=64, epochs=2, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.3458220064640045, 0.16459320485591888],\n",
       " 'sparse_categorical_accuracy': [0.9023000001907349, 0.9518600106239319],\n",
       " 'val_loss': [0.19515098631381989, 0.14224953949451447],\n",
       " 'val_sparse_categorical_accuracy': [0.9451000094413757, 0.9587000012397766]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# history object holds the metrics and losses for each epoch of both training and validation data\n",
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evauae on Test Data\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.1538 - sparse_categorical_accuracy: 0.9558\n",
      "test loss, test acc: [0.15379703044891357, 0.9557999968528748]\n",
      "Generate Predictions for 3 samples\n",
      "1/1 [==============================] - 0s 335ms/step\n",
      "predictions shape (3, 10)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data using \"evaluate\" method\n",
    "print(\"Evauae on Test Data\")\n",
    "results = model.evaluate(x_test, y_test, batch_size=128)\n",
    "print(\"test loss, test acc:\", results)\n",
    "# Get predictions on individual images or batch of images using predict method\n",
    "print(\"Generate Predictions for 3 samples\")\n",
    "predictions = model.predict(x_test[:3])\n",
    "print(\"predictions shape\", predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling a Model: Loss, Metrics, Optimizer\n",
    "To train a model before going to fit() we need to compile the model with following fields\n",
    "- optimizer - Algorithm for Backpropogation (example: Adam, RMSProp, Adagrad,...etc)\n",
    "- loss - If the model have multiple outputs then we can specify different loss functions for each output\n",
    "- metrics - its list of where we can specify any number of metrics. and also for multi output model we can specify multiple types of metrics\n",
    "\n",
    "If we want to go with default values for (optimizer, loss and metrics) we can specify them in strings. if we want to customize them we need to call respective functions from keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model compilation with default fileds\n",
    "model.compile(\n",
    "    optimizer=\"rmsprop\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"sparse_categorical_accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model compilation with custom functions\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For later reuse, let's put our model definition and compile step in functions; we will call them several times across different examples in this guide.\n",
    "def get_uncompiled_model():\n",
    "    inputs = keras.Input(shape=(784,), name=\"digits\")\n",
    "    x = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs)\n",
    "    x = layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x)\n",
    "    outputs = layers.Dense(10, activation=\"softmax\", name=\"predictions\")(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_compiled_model():\n",
    "    model = get_uncompiled_model()\n",
    "    model.compile(\n",
    "        optimizer=\"rmsprop\",\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"sparse_categorical_accuracy\"],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Builtin Optimizers, Losses and Metircs\n",
    "Optimizers: tf.keras.optimizers.\n",
    "- SGD() (with or without momentum)\n",
    "- RMSProp()\n",
    "- Adam()\n",
    "- Adagard()\n",
    "- Adadelta()\n",
    "- Adamax()\n",
    "\n",
    "Losses: tf.keras.losses.\n",
    "- BinaryCrossentropy()\n",
    "- CategoricalCrossentropy()\n",
    "- CategoricalHinge()\n",
    "- CosineSimilairty()\n",
    "- Hinge()\n",
    "- KLDivergence()\n",
    "- MeanAbsoluteError()\n",
    "- MeanAbsolutePercentageError()\n",
    "- MeanSquaredError()\n",
    "- MeanSquaredLogarithmicError()\n",
    "- SparseCategoricalCrossentropy()\n",
    "\n",
    "Metrics: tf.keras.metrics.\n",
    "- AUC()\n",
    "- Precision()\n",
    "- Recall()\n",
    "- Accuracy()\n",
    "\n",
    "Apart from there if we want to create a custom functions Keras has the feasability to create\n",
    "\n",
    "### Custom Loss:\n",
    "To create a custom loss function we can do it in two ways\n",
    "- create a function which takes y_true and y_pred as inputs (it wont accept other inputs)\n",
    "- if we want to have other paramters along with y_true and y_pred we need to create a custom loss class inherited from tf.keras.losses.Loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 6s 5ms/step - loss: 0.0157\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23432192b20>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The following example shows a loss function that computes the mean squared error between the real data and the predictions:\n",
    "def custom_mean_squared_error(y_true, y_pred):\n",
    "    return tf.math.reduce_mean(tf.square(y_true - y_pred))\n",
    "\n",
    "\n",
    "model = get_uncompiled_model()\n",
    "model.compile(optimizer=keras.optimizers.Adam(), loss=custom_mean_squared_error)\n",
    "\n",
    "# We need to one-hot encode the labels to use MSE\n",
    "y_train_one_hot = tf.one_hot(y_train, depth=10)\n",
    "model.fit(x_train, y_train_one_hot, batch_size=64, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need a loss function that takes in parameters beside y_true and y_pred, you can subclass the tf.keras.losses.Loss class and implement the following two methods:\n",
    "- __init__(self): accept parameters to pass during the call of your loss function\n",
    "- call(self, y_true, y_pred): use the targets (y_true) and the model predictions (y_pred) to compute the model's loss\n",
    "\n",
    "Let's say you want to use mean squared error, but with an added term that will de-incentivize prediction values far from 0.5 (we assume that the categorical targets are one-hot encoded and take values between 0 and 1). This creates an incentive for the model not to be too confident, which may help reduce overfitting (we won't know if it works until we try!).\n",
    "\n",
    "Here's how you would do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 7s 6ms/step - loss: 0.0384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23434d80b80>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CustomMSE(tf.keras.losses.Loss):\n",
    "    def __init__(self, regularization_factor=0.1, name=\"custom_mse\"):\n",
    "        super(CustomMSE, self).__init__(name=name)\n",
    "        self.regularization_factor = regularization_factor\n",
    "    def call(self, y_true, y_pred):\n",
    "        mse = tf.reduce_mean(tf.square(y_true-y_pred))\n",
    "        reg = tf.math.reduce_mean(tf.square(0.5 - y_pred))\n",
    "        return mse+reg*self.regularization_factor\n",
    "\n",
    "model = get_uncompiled_model()\n",
    "model.compile(optimizer=keras.optimizers.Adam(), loss=CustomMSE())\n",
    "\n",
    "y_train_one_hot = tf.one_hot(y_train, depth=10)\n",
    "model.fit(x_train, y_train_one_hot, batch_size=64, epochs=1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Metrics:\n",
    "if we want to create or use any metric which is not a part of API we can create it by using tf.keras.metrics.Metric class. For this we need to implement 4 methods:\n",
    "- __init__(self) in which we create state variables for our metric\n",
    "- update_state(self, y_true, y_pred, sample_weight=None) which takes y_true and y_pred to update the sate varaibles\n",
    "- result(self) which uses the state varaible to compute the result\n",
    "- reset_state(Self) which reinitializes the state of the metric\n",
    "\n",
    "State update and result computations kept separately because when the data size used for the results computation is vere huge, it will become computationally very expensive and would be done periodically. So for each period state varaibles will be updated and corresponing results as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "782/782 [==============================] - 6s 5ms/step - loss: 0.3513 - categorical_true_positives: 45001.0000\n",
      "Epoch 2/3\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.1628 - categorical_true_positives: 47567.0000\n",
      "Epoch 3/3\n",
      "782/782 [==============================] - 5s 7ms/step - loss: 0.1208 - categorical_true_positives: 48202.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x234350597c0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here's a simple example showing how to implement a CategoricalTruePositives metric that counts how many samples were correctly classified as belonging to a given class:\n",
    "class CategoricalTruePositives(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name=\"categorical_true_positives\", **kwargs):\n",
    "        super(CategoricalTruePositives, self).__init__(name=name, **kwargs)\n",
    "        self.true_positives = self.add_weight(name=\"ctp\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = tf.reshape(tf.argmax(y_pred, axis=1), (-1,1))\n",
    "        values = tf.cast(y_true, \"int32\") == tf.cast(y_pred, \"int32\")\n",
    "        values = tf.cast(values, \"float32\")\n",
    "        if sample_weight is not None:\n",
    "            sample_weight = tf.cast(sample_weight, \"float32\")\n",
    "            values = tf.multiply(values, sample_weight)\n",
    "        self.true_positives.assign_add(tf.reduce_sum(values))\n",
    "\n",
    "    def result(self):\n",
    "        return self.true_positives\n",
    "    \n",
    "    def reset_state(self):\n",
    "        # The state of the metric will be reset at the start of each epoch.\n",
    "        self.true_positives.assign(0.0)\n",
    "\n",
    "model = get_uncompiled_model()\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=[CategoricalTruePositives()],\n",
    ")\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Losses Metrics that don' fit the standard signature\n",
    "- Many of losses and metrics can be computed using y_true and y_pred\n",
    "- But in some cases model output might not be used to compute the loss\n",
    "- For example a regularization loss may only require the activation of a layer, and this activation may not be a model output.\n",
    "- In such cases, you can call self.add_loss(loss_value) from inside the call method of a custom layer. \n",
    "- Losses added in this way get added to the \"main\" loss during training (the one passed to compile())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 6s 6ms/step - loss: 2.5193\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23434f04a00>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ActivityRegularizationLayer(tf.keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        self.add_loss(tf.reduce_sum(inputs)*0.1)\n",
    "        return inputs\n",
    "\n",
    "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
    "x = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs)\n",
    "\n",
    "# Insert activity regularization as a layer\n",
    "x = ActivityRegularizationLayer()(x)\n",
    "x = layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x)\n",
    "outputs = layers.Dense(10, name=\"predictions\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    ")\n",
    "\n",
    "# The displayed loss will be much higher than before\n",
    "# due to the regularization component.\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 6s 5ms/step - loss: 0.3353 - std_of_activation: 0.9592\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x234354fc100>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can do the same for logging metric values, using add_metric()\n",
    "class MetricLoggingLayer(tf.keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        # The `aggregation` argument defines\n",
    "        # how to aggregate the per-batch values\n",
    "        # over each epoch:\n",
    "        # in this case we simply average them.\n",
    "        self.add_metric(\n",
    "            tf.keras.backend.std(inputs),name=\"std_of_activation\", aggregation=\"mean\"\n",
    "        )\n",
    "        return inputs\n",
    "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
    "x = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs)\n",
    "\n",
    "# Insert std logging as a layer.\n",
    "x = MetricLoggingLayer()(x)\n",
    "\n",
    "x = layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x)\n",
    "outputs = layers.Dense(10, name=\"predictions\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    ")\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 7s 8ms/step - loss: 2.5005 - std_of_activation: 0.0019\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23439cbe130>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In the Functional API, you can also call model.add_loss(loss_tensor), or model.add_metric(metric_tensor, name, aggregation).\n",
    "# Here's a simple example:\n",
    "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
    "x1 = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs)\n",
    "x2 = layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x1)\n",
    "outputs = layers.Dense(10, name=\"predictions\")(x2)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.add_loss(tf.reduce_sum(x1) * 0.1)\n",
    "\n",
    "model.add_metric(keras.backend.std(x1), name=\"std_of_activation\", aggregation=\"mean\")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    ")\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when you pass losses via add_loss(), it becomes possible to call compile() without a loss function, since the model already has a loss to minimize.\n",
    "\n",
    "Consider the following LogisticEndpoint layer: it takes as inputs targets & logits, and it tracks a crossentropy loss via add_loss(). It also tracks classification accuracy via add_metric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step - loss: 1.0304 - binary_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2343a6fcfa0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LogisticEndpoint(tf.keras.layers.Layer):\n",
    "    def __init__(self, name=None):\n",
    "        super(LogisticEndpoint, self).__init__(name=name)\n",
    "        self.loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "        self.accuracy_fn = tf.keras.metrics.BinaryAccuracy()\n",
    "\n",
    "    def call(self, targets, logits, sample_weights=None):\n",
    "        # Compute the training-time loss value and add it\n",
    "        # to the layer using `self.add_loss()`.\n",
    "        loss = self.loss_fn(targets, logits, sample_weights)\n",
    "        self.add_loss(loss)\n",
    "        # Log accuracy as a metric and add it\n",
    "        # to the layer using `self.add_metric()`.\n",
    "        acc = self.accuracy_fn(targets, logits, sample_weights)\n",
    "        self.add_metric(acc, name=\"accuracy\")\n",
    "        # Return the inference-time prediction tensor (for `.predict()`).\n",
    "        return tf.nn.softmax(logits)\n",
    "\n",
    "#you can use it in a model with two inputs (input data & targets), compiled without a loss argument, like this:\n",
    "inputs = keras.Input(shape=(3,), name=\"inputs\")\n",
    "targets = keras.Input(shape=(10,), name=\"targets\")\n",
    "logits = keras.layers.Dense(10)(inputs)\n",
    "predictions = LogisticEndpoint(name=\"predictions\")(logits, targets)\n",
    "\n",
    "model = keras.Model(inputs=[inputs, targets], outputs=predictions)\n",
    "model.compile(optimizer=\"adam\")  # No loss argument!\n",
    "\n",
    "data = {\n",
    "    \"inputs\": np.random.random((3, 3)),\n",
    "    \"targets\": np.random.random((3, 10)),\n",
    "}\n",
    "model.fit(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatically Setting Apart a Validation Holdout Set\n",
    "- So far we have used validation data set for validating the model while training\n",
    "- Instead we can split the training data into vaidation while training using validation_split paramters\n",
    "- But this can be used only when we are using Numpy data for training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 6s 8ms/step - loss: 0.3638 - sparse_categorical_accuracy: 0.8982 - val_loss: 0.2209 - val_sparse_categorical_accuracy: 0.9367\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23434ce9b50>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_compiled_model()\n",
    "model.fit(x_train, y_train, batch_size=64, validation_split=0.2, epochs=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation from tf.data Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "127b1e9fcfedd5dda8ac936388b1f24776168c431ca343c8354d3bc75daf36ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
