{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "- We use three Keras functionalities (inherits from keras model) to train, evaluate and pedict from the model that developed\n",
    "    - Model.fit() - for training\n",
    "    - Model.evaluate() - to evaluate the model\n",
    "    - Model.predict() - to infere the model on test example or test dataset\n",
    "- When we use built in loops for training and evaluation, process will be same for both Seqential and Functional API models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A First end -to-end Example\n",
    "- Data can be fed to training loops either using\n",
    "    - Numpy Arrays (When the data is small and can be fit into memory)\n",
    "    - tf.data Dataset objects\n",
    "\n",
    "Lets consider the following model for MNIST classification:\n",
    "\n",
    "A Typical end-to-end workflow looks like consists of:\n",
    "- Training\n",
    "- Validation on Hold out Data (generated from original training data)\n",
    "- Evaluation on Test Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
    "x = layers.Dense(64, activation=\"relu\", name=\"dense1\")(inputs)\n",
    "x = layers.Dense(64, activation=\"relu\", name=\"dense2\")(x)\n",
    "outputs = layers.Dense(10, activation=\"softmax\", name=\"predictio\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training The Model...\n",
      "Epoch 1/2\n",
      "782/782 [==============================] - 9s 9ms/step - loss: 0.3341 - sparse_categorical_accuracy: 0.9040 - val_loss: 0.1738 - val_sparse_categorical_accuracy: 0.9493\n",
      "Epoch 2/2\n",
      "782/782 [==============================] - 5s 7ms/step - loss: 0.1588 - sparse_categorical_accuracy: 0.9525 - val_loss: 0.1383 - val_sparse_categorical_accuracy: 0.9573\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Preprocess the data\n",
    "x_train = x_train.reshape(-1,784).astype(\"float32\")/255\n",
    "x_test = x_test.reshape(-1,784).astype(\"float32\")/255\n",
    "\n",
    "y_train = y_train.astype(\"float32\")\n",
    "y_test = y_test.astype(\"float32\")\n",
    "\n",
    "# Reserve 10000 samples for validation\n",
    "x_val = x_train[-10000:]\n",
    "y_val = y_train[-10000:]\n",
    "x_train = x_train[:-10000]\n",
    "y_train = y_train[:-10000]\n",
    "\n",
    "# Specify the training configuration (optimizser, loss, metrics)\n",
    "model.compile(optimizer = keras.optimizers.RMSprop(),\n",
    "              loss = keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics = [keras.metrics.SparseCategoricalAccuracy()],)\n",
    "\n",
    "# fit() is for training the model with several parameters\n",
    "print(\"Training The Model...\")\n",
    "history = model.fit(x_train, y_train, batch_size=64, epochs=2, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.334058940410614, 0.158828005194664],\n",
       " 'sparse_categorical_accuracy': [0.903980016708374, 0.952459990978241],\n",
       " 'val_loss': [0.17381303012371063, 0.13834749162197113],\n",
       " 'val_sparse_categorical_accuracy': [0.9492999911308289, 0.9573000073432922]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# history object holds the metrics and losses for each epoch of both training and validation data\n",
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evauae on Test Data\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.1507 - sparse_categorical_accuracy: 0.9519\n",
      "test loss, test acc: [0.15071482956409454, 0.9519000053405762]\n",
      "Generate Predictions for 3 samples\n",
      "1/1 [==============================] - 0s 190ms/step\n",
      "predictions shape (3, 10)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data using \"evaluate\" method\n",
    "print(\"Evauae on Test Data\")\n",
    "results = model.evaluate(x_test, y_test, batch_size=128)\n",
    "print(\"test loss, test acc:\", results)\n",
    "# Get predictions on individual images or batch of images using predict method\n",
    "print(\"Generate Predictions for 3 samples\")\n",
    "predictions = model.predict(x_test[:3])\n",
    "print(\"predictions shape\", predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling a Model: Loss, Metrics, Optimizer\n",
    "To train a model before going to fit() we need to compile the model with following fields\n",
    "- optimizer - Algorithm for Backpropogation (example: Adam, RMSProp, Adagrad,...etc)\n",
    "- loss - If the model have multiple outputs then we can specify different loss functions for each output\n",
    "- metrics - its list of where we can specify any number of metrics. and also for multi output model we can specify multiple types of metrics\n",
    "\n",
    "If we want to go with default values for (optimizer, loss and metrics) we can specify them in strings. if we want to customize them we need to call respective functions from keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model compilation with default fileds\n",
    "model.compile(\n",
    "    optimizer=\"rmsprop\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"sparse_categorical_accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model compilation with custom functions\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For later reuse, let's put our model definition and compile step in functions; we will call them several times across different examples in this guide.\n",
    "def get_uncompiled_model():\n",
    "    inputs = keras.Input(shape=(784,), name=\"digits\")\n",
    "    x = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs)\n",
    "    x = layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x)\n",
    "    outputs = layers.Dense(10, activation=\"softmax\", name=\"predictions\")(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_compiled_model():\n",
    "    model = get_uncompiled_model()\n",
    "    model.compile(\n",
    "        optimizer=\"rmsprop\",\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"sparse_categorical_accuracy\"],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Builtin Optimizers, Losses and Metircs\n",
    "Optimizers: tf.keras.optimizers.\n",
    "- SGD() (with or without momentum)\n",
    "- RMSProp()\n",
    "- Adam()\n",
    "- Adagard()\n",
    "- Adadelta()\n",
    "- Adamax()\n",
    "\n",
    "Losses: tf.keras.losses.\n",
    "- BinaryCrossentropy()\n",
    "- CategoricalCrossentropy()\n",
    "- CategoricalHinge()\n",
    "- CosineSimilairty()\n",
    "- Hinge()\n",
    "- KLDivergence()\n",
    "- MeanAbsoluteError()\n",
    "- MeanAbsolutePercentageError()\n",
    "- MeanSquaredError()\n",
    "- MeanSquaredLogarithmicError()\n",
    "- SparseCategoricalCrossentropy()\n",
    "\n",
    "Metrics: tf.keras.metrics.\n",
    "- AUC()\n",
    "- Precision()\n",
    "- Recall()\n",
    "- Accuracy()\n",
    "\n",
    "Apart from there if we want to create a custom functions Keras has the feasability to create\n",
    "\n",
    "### Custom Loss:\n",
    "To create a custom loss function we can do it in two ways\n",
    "- create a function which takes y_true and y_pred as inputs (it wont accept other inputs)\n",
    "- if we want to have other paramters along with y_true and y_pred we need to create a custom loss class inherited from tf.keras.losses.Loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 6s 6ms/step - loss: 0.0161\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a8e80cc220>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The following example shows a loss function that computes the mean squared error between the real data and the predictions:\n",
    "def custom_mean_squared_error(y_true, y_pred):\n",
    "    return tf.math.reduce_mean(tf.square(y_true - y_pred))\n",
    "\n",
    "\n",
    "model = get_uncompiled_model()\n",
    "model.compile(optimizer=keras.optimizers.Adam(), loss=custom_mean_squared_error)\n",
    "\n",
    "# We need to one-hot encode the labels to use MSE\n",
    "y_train_one_hot = tf.one_hot(y_train, depth=10)\n",
    "model.fit(x_train, y_train_one_hot, batch_size=64, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need a loss function that takes in parameters beside y_true and y_pred, you can subclass the tf.keras.losses.Loss class and implement the following two methods:\n",
    "- __init__(self): accept parameters to pass during the call of your loss function\n",
    "- call(self, y_true, y_pred): use the targets (y_true) and the model predictions (y_pred) to compute the model's loss\n",
    "\n",
    "Let's say you want to use mean squared error, but with an added term that will de-incentivize prediction values far from 0.5 (we assume that the categorical targets are one-hot encoded and take values between 0 and 1). This creates an incentive for the model not to be too confident, which may help reduce overfitting (we won't know if it works until we try!).\n",
    "\n",
    "Here's how you would do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 10s 10ms/step - loss: 0.0388\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a8e9b68b50>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CustomMSE(tf.keras.losses.Loss):\n",
    "    def __init__(self, regularization_factor=0.1, name=\"custom_mse\"):\n",
    "        super(CustomMSE, self).__init__(name=name)\n",
    "        self.regularization_factor = regularization_factor\n",
    "    def call(self, y_true, y_pred):\n",
    "        mse = tf.reduce_mean(tf.square(y_true-y_pred))\n",
    "        reg = tf.math.reduce_mean(tf.square(0.5 - y_pred))\n",
    "        return mse+reg*self.regularization_factor\n",
    "\n",
    "model = get_uncompiled_model()\n",
    "model.compile(optimizer=keras.optimizers.Adam(), loss=CustomMSE())\n",
    "\n",
    "y_train_one_hot = tf.one_hot(y_train, depth=10)\n",
    "model.fit(x_train, y_train_one_hot, batch_size=64, epochs=1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Metrics:\n",
    "if we want to create or use any metric which is not a part of API we can create it by using tf.keras.metrics.Metric class. For this we need to implement 4 methods:\n",
    "- __init__(self) in which we create state variables for our metric\n",
    "- update_state(self, y_true, y_pred, sample_weight=None) which takes y_true and y_pred to update the sate varaibles\n",
    "- result(self) which uses the state varaible to compute the result\n",
    "- reset_state(Self) which reinitializes the state of the metric\n",
    "\n",
    "State update and result computations kept separately because when the data size used for the results computation is vere huge, it will become computationally very expensive and would be done periodically. So for each period state varaibles will be updated and corresponing results as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "782/782 [==============================] - 10s 10ms/step - loss: 0.3404 - categorical_true_positives: 45231.0000\n",
      "Epoch 2/3\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 0.1602 - categorical_true_positives: 47658.0000\n",
      "Epoch 3/3\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 0.1177 - categorical_true_positives: 48195.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a8e9c5ac40>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here's a simple example showing how to implement a CategoricalTruePositives metric that counts how many samples were correctly classified as belonging to a given class:\n",
    "class CategoricalTruePositives(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name=\"categorical_true_positives\", **kwargs):\n",
    "        super(CategoricalTruePositives, self).__init__(name=name, **kwargs)\n",
    "        self.true_positives = self.add_weight(name=\"ctp\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = tf.reshape(tf.argmax(y_pred, axis=1), (-1,1))\n",
    "        values = tf.cast(y_true, \"int32\") == tf.cast(y_pred, \"int32\")\n",
    "        values = tf.cast(values, \"float32\")\n",
    "        if sample_weight is not None:\n",
    "            sample_weight = tf.cast(sample_weight, \"float32\")\n",
    "            values = tf.multiply(values, sample_weight)\n",
    "        self.true_positives.assign_add(tf.reduce_sum(values))\n",
    "\n",
    "    def result(self):\n",
    "        return self.true_positives\n",
    "    \n",
    "    def reset_state(self):\n",
    "        # The state of the metric will be reset at the start of each epoch.\n",
    "        self.true_positives.assign(0.0)\n",
    "\n",
    "model = get_uncompiled_model()\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=[CategoricalTruePositives()],\n",
    ")\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Losses Metrics that don' fit the standard signature\n",
    "- Many of losses and metrics can be computed using y_true and y_pred\n",
    "- But in some cases model output might not be used to compute the loss\n",
    "- For example a regularization loss may only require the activation of a layer, and this activation may not be a model output.\n",
    "- In such cases, you can call self.add_loss(loss_value) from inside the call method of a custom layer. \n",
    "- Losses added in this way get added to the \"main\" loss during training (the one passed to compile())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 7s 6ms/step - loss: 2.4647\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a88119bd30>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ActivityRegularizationLayer(tf.keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        self.add_loss(tf.reduce_sum(inputs)*0.1)\n",
    "        return inputs\n",
    "\n",
    "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
    "x = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs)\n",
    "\n",
    "# Insert activity regularization as a layer\n",
    "x = ActivityRegularizationLayer()(x)\n",
    "x = layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x)\n",
    "outputs = layers.Dense(10, name=\"predictions\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    ")\n",
    "\n",
    "# The displayed loss will be much higher than before\n",
    "# due to the regularization component.\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 10s 10ms/step - loss: 0.3440 - std_of_activation: 0.9869\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a88141c3a0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can do the same for logging metric values, using add_metric()\n",
    "class MetricLoggingLayer(tf.keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        # The `aggregation` argument defines\n",
    "        # how to aggregate the per-batch values\n",
    "        # over each epoch:\n",
    "        # in this case we simply average them.\n",
    "        self.add_metric(\n",
    "            tf.keras.backend.std(inputs),name=\"std_of_activation\", aggregation=\"mean\"\n",
    "        )\n",
    "        return inputs\n",
    "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
    "x = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs)\n",
    "\n",
    "# Insert std logging as a layer.\n",
    "x = MetricLoggingLayer()(x)\n",
    "\n",
    "x = layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x)\n",
    "outputs = layers.Dense(10, name=\"predictions\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    ")\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 8s 7ms/step - loss: 2.4933 - std_of_activation: 0.0019\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a8826c1ca0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In the Functional API, you can also call model.add_loss(loss_tensor), or model.add_metric(metric_tensor, name, aggregation).\n",
    "# Here's a simple example:\n",
    "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
    "x1 = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs)\n",
    "x2 = layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x1)\n",
    "outputs = layers.Dense(10, name=\"predictions\")(x2)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.add_loss(tf.reduce_sum(x1) * 0.1)\n",
    "\n",
    "model.add_metric(keras.backend.std(x1), name=\"std_of_activation\", aggregation=\"mean\")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    ")\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when you pass losses via add_loss(), it becomes possible to call compile() without a loss function, since the model already has a loss to minimize.\n",
    "\n",
    "Consider the following LogisticEndpoint layer: it takes as inputs targets & logits, and it tracks a crossentropy loss via add_loss(). It also tracks classification accuracy via add_metric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step - loss: 0.8991 - binary_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a8839bc070>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LogisticEndpoint(tf.keras.layers.Layer):\n",
    "    def __init__(self, name=None):\n",
    "        super(LogisticEndpoint, self).__init__(name=name)\n",
    "        self.loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "        self.accuracy_fn = tf.keras.metrics.BinaryAccuracy()\n",
    "\n",
    "    def call(self, targets, logits, sample_weights=None):\n",
    "        # Compute the training-time loss value and add it\n",
    "        # to the layer using `self.add_loss()`.\n",
    "        loss = self.loss_fn(targets, logits, sample_weights)\n",
    "        self.add_loss(loss)\n",
    "        # Log accuracy as a metric and add it\n",
    "        # to the layer using `self.add_metric()`.\n",
    "        acc = self.accuracy_fn(targets, logits, sample_weights)\n",
    "        self.add_metric(acc, name=\"accuracy\")\n",
    "        # Return the inference-time prediction tensor (for `.predict()`).\n",
    "        return tf.nn.softmax(logits)\n",
    "\n",
    "#you can use it in a model with two inputs (input data & targets), compiled without a loss argument, like this:\n",
    "inputs = keras.Input(shape=(3,), name=\"inputs\")\n",
    "targets = keras.Input(shape=(10,), name=\"targets\")\n",
    "logits = keras.layers.Dense(10)(inputs)\n",
    "predictions = LogisticEndpoint(name=\"predictions\")(logits, targets)\n",
    "\n",
    "model = keras.Model(inputs=[inputs, targets], outputs=predictions)\n",
    "model.compile(optimizer=\"adam\")  # No loss argument!\n",
    "\n",
    "data = {\n",
    "    \"inputs\": np.random.random((3, 3)),\n",
    "    \"targets\": np.random.random((3, 10)),\n",
    "}\n",
    "model.fit(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatically Setting Apart a Validation Holdout Set\n",
    "- So far we have used validation data set for validating the model while training\n",
    "- Instead we can split the training data into vaidation while training using validation_split paramters\n",
    "- But this can be used only when we are using Numpy data for training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 9s 11ms/step - loss: 0.3671 - sparse_categorical_accuracy: 0.8975 - val_loss: 0.2231 - val_sparse_categorical_accuracy: 0.9305\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a8e9c718b0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_compiled_model()\n",
    "model.fit(x_train, y_train, batch_size=64, validation_split=0.2, epochs=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation from tf.data Datasets\n",
    "- So far we worked with Numpy array datasets\n",
    "- Now lets look at the case where our data comes in the form of tf.data.Dataset object.\n",
    "- The tf.data API is a set of utilities in Tensorflow 2.0 for loading and preprocessing data in a way that's fast and scalable\n",
    "- We cah pass a Dataset instance directly to the methods fit(), evaluate() and predict():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "782/782 [==============================] - 7s 6ms/step - loss: 0.3319 - sparse_categorical_accuracy: 0.9059\n",
      "Epoch 2/3\n",
      "782/782 [==============================] - 5s 7ms/step - loss: 0.1567 - sparse_categorical_accuracy: 0.9529\n",
      "Epoch 3/3\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 0.1149 - sparse_categorical_accuracy: 0.9662\n",
      "Evaluate...\n",
      "157/157 [==============================] - 1s 4ms/step - loss: 0.1108 - sparse_categorical_accuracy: 0.9672\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 0.11077859252691269, 'sparse_categorical_accuracy': 0.967199981212616}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_compiled_model()\n",
    "# Lets create a Datset instance on MNIST data \n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "# shuffle and slice the datset\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "# Now do the same for test dataset\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_dataset = test_dataset.batch(64)\n",
    "\n",
    "# since we passed batching in dataset there is no need of providing batch_size in training\n",
    "model.fit(train_dataset, epochs=3)\n",
    "\n",
    "# lets evaluate on test dataset\n",
    "print(\"Evaluate...\")\n",
    "result = model.evaluate(test_dataset)\n",
    "dict(zip(model.metrics_names,result))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dataset will be reset at the end of the each epoch, so that it can be reused in the next epoch. \n",
    "- If we want to run only a specific numver of batches for each epoch we can pass the argument steps_per_epoch\n",
    "- This argument specifies howmany training stpes the model should run using the Dataset before moving on to the next epoch. \n",
    "- If we do this the Dataset is not reset at the end of the each epoch, insted we just keep drawing the next batches. \n",
    "- The dataset will eventually run outof data (unless it is an infinitely-looping dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/11\n",
      "100/100 [==============================] - 2s 5ms/step - loss: 0.7853 - sparse_categorical_accuracy: 0.7962\n",
      "Epoch 2/11\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.3704 - sparse_categorical_accuracy: 0.8955\n",
      "Epoch 3/11\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.3210 - sparse_categorical_accuracy: 0.9047\n",
      "Epoch 4/11\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.2583 - sparse_categorical_accuracy: 0.9267\n",
      "Epoch 5/11\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 0.2613 - sparse_categorical_accuracy: 0.9237\n",
      "Epoch 6/11\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 0.2124 - sparse_categorical_accuracy: 0.9395\n",
      "Epoch 7/11\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.2192 - sparse_categorical_accuracy: 0.9356\n",
      "Epoch 8/11\n",
      " 74/100 [=====================>........] - ETA: 0s - loss: 0.2087 - sparse_categorical_accuracy: 0.9352WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 1100 batches). You may need to use the repeat() function when building your dataset.\n",
      "100/100 [==============================] - 3s 35ms/step - loss: 0.2080 - sparse_categorical_accuracy: 0.9346\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a884dca4c0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_compiled_model()\n",
    "# prepare the training dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "# only use 1000 vatches per epoch (thats 64*100 samples)\n",
    "model.fit(train_dataset, epochs=11, steps_per_epoch = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/11\n",
      "100/100 [==============================] - 3s 6ms/step - loss: 0.7777 - sparse_categorical_accuracy: 0.7906\n",
      "Epoch 2/11\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.3769 - sparse_categorical_accuracy: 0.8923\n",
      "Epoch 3/11\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.3269 - sparse_categorical_accuracy: 0.9039\n",
      "Epoch 4/11\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 0.2751 - sparse_categorical_accuracy: 0.9191\n",
      "Epoch 5/11\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.2644 - sparse_categorical_accuracy: 0.9200\n",
      "Epoch 6/11\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.2316 - sparse_categorical_accuracy: 0.9308\n",
      "Epoch 7/11\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.2291 - sparse_categorical_accuracy: 0.9322\n",
      "Epoch 8/11\n",
      "100/100 [==============================] - 1s 14ms/step - loss: 0.2228 - sparse_categorical_accuracy: 0.9353\n",
      "Epoch 9/11\n",
      "100/100 [==============================] - 2s 17ms/step - loss: 0.1710 - sparse_categorical_accuracy: 0.9525\n",
      "Epoch 10/11\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.1817 - sparse_categorical_accuracy: 0.9467\n",
      "Epoch 11/11\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.1664 - sparse_categorical_accuracy: 0.9508\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a885906d60>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_compiled_model()\n",
    "# prepare the training dataset using repeat function\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).repeat()\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "# only use 1000 vatches per epoch (thats 64*100 samples)\n",
    "model.fit(train_dataset, epochs=11, steps_per_epoch = 100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a Validation Dataset\n",
    "- We can pass a Dataset instance as a validation_data argument in fit():\n",
    "- At the end of each epoch the model will iterate over the validation dataset and compute the validation loss and validation metrics\n",
    "- If we want to run validation only on a specific number of batches from this dataset, we can pass the validation_steps argument\n",
    "- this argument specifies how many validation steps the model should run with the validation dataset before interrupting validation and moving on to the next epoch.\n",
    "- Note that the validation dataset will be reset after each use even with validation_steps argument (so that you will always be evaluating on the same samples from epoch to epoch).\n",
    "- The argument validation_split (generating a holdout set from the training data) is not supported when training from Dataset objects, since this feature requires the ability to index the samples of the datasets, which is not possible in general with the Dataset API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 9s 9ms/step - loss: 0.3357 - sparse_categorical_accuracy: 0.9064 - val_loss: 0.1917 - val_sparse_categorical_accuracy: 0.9455\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a887072340>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_compiled_model()\n",
    "\n",
    "# Prepare the training dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "# Prepare the validation dataset\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_dataset = val_dataset.batch(64)\n",
    "\n",
    "model.fit(train_dataset, epochs=1, validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 8s 8ms/step - loss: 0.3329 - sparse_categorical_accuracy: 0.9063 - val_loss: 0.2805 - val_sparse_categorical_accuracy: 0.9203\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a886dff640>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_compiled_model()\n",
    "\n",
    "# Prepare the training dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "# Prepare the validation dataset\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_dataset = val_dataset.batch(64)\n",
    "\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    epochs=1,\n",
    "    # Only run validation using the first 10 batches of the dataset\n",
    "    # using the `validation_steps` argument\n",
    "    validation_data=val_dataset,\n",
    "    validation_steps=10,\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uisng tf.keras.uitls.Sequence Object as input\n",
    "- Besides NumPy arrays, eager tensors, and TensorFlow Datasets, it's possible to train a Keras model using Pandas dataframes, or from Python generators that yield batches of data & labels.\n",
    "- In particular, the keras.utils.Sequence class offers a simple interface to build Python data generators that are multiprocessing-aware and can be shuffled.\n",
    "- In general, we recommend that you use:\n",
    "    - NumPy input data if your data is small and fits in memory\n",
    "    - Dataset objects if you have large datasets and you need to do distributed training\n",
    "    - Sequence objects if you have large datasets and you need to do a lot of custom Python-side processing that cannot be done in TensorFlow (e.g. if you rely on external libraries for data loading or preprocessing).\n",
    "\n",
    "\n",
    "keras.utils.Sequence is a utility that you can subclass to obtain a Python generator with two important properties:\n",
    "- It works well with multiprocessing.\n",
    "- It can be shuffled (e.g. when passing shuffle=True in fit()).\n",
    "\n",
    "A Sequence must implement two methods:\n",
    "- \\_\\_getitem\\_\\_\n",
    "- \\_\\_len\\_\\_\n",
    "\n",
    "The method __getitem__ should return a complete batch. If you want to modify your dataset between epochs, you may implement on_epoch_end.\n",
    "\n",
    "Here's a quick example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Here, `filenames` is list of path to the images\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m# and `labels` are the associated labels.\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mCIFAR10Sequence\u001b[39;00m(tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mSequence):\n\u001b[0;32m      4\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, filenames, labels, batch_size):\n\u001b[0;32m      5\u001b[0m         \u001b[39msuper\u001b[39m(CIFAR10Sequence, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "# Here, `filenames` is list of path to the images\n",
    "# and `labels` are the associated labels.\n",
    "class CIFAR10Sequence(tf.keras.utils.Sequence):\n",
    "    def __init__(self, filenames, labels, batch_size):\n",
    "        super(CIFAR10Sequence, self).__init__()\n",
    "        self.filenames = filenames\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "    def __len__(self):\n",
    "        return len(np.ceil(len(self.filenames)/float(self.batch_size)))\n",
    "    def __getitem__(self,idx):\n",
    "        batch_x = self.filenames[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.labels[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        return np.array([resize(imread(filename), (200, 200)) for filename in batch_x]), np.array(batch_y)\n",
    "\n",
    "#sequence = CIFAR10Sequence(filenames, labels, batch_size)\n",
    "#model.fit(sequence, epochs=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using sample weighting and class weighting\n",
    "With the default settings the weight of a sample is decided by its frequency in the dataset. There are two methods to weight the data, independent of sample frequency:\n",
    "- Class weights\n",
    "- Sample weights\n",
    "\n",
    "### Class weights\n",
    "- This is set by passing a dictionary to the class_weight argument to Model.fit(). \n",
    "- This dictionary maps class indices to the weight that should be used for samples belonging to this class.\n",
    "- This can be used to balance classes without resampling, or to train a model that gives more importance to a particular class.\n",
    "- For instance, if class \"0\" is half as represented as class \"1\" in your data, you could use Model.fit(..., class_weight={0: 1., 1: 0.5}).\n",
    "\n",
    "Here's a NumPy example where we use class weights or sample weights to give more importance to the correct classification of class #5 (which is the digit \"5\" in the MNIST dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class_weight = {\n",
    "    0: 1.0,\n",
    "    1: 1.0,\n",
    "    2: 1.0,\n",
    "    3: 1.0,\n",
    "    4: 1.0,\n",
    "    # Set weight \"2\" for class \"5\",\n",
    "    # making this class 2x more important\n",
    "    5: 2.0,\n",
    "    6: 1.0,\n",
    "    7: 1.0,\n",
    "    8: 1.0,\n",
    "    9: 1.0,\n",
    "}\n",
    "\n",
    "print(\"Fit with class weight\")\n",
    "model = get_compiled_model()\n",
    "model.fit(x_train, y_train, class_weight=class_weight, batch_size=64, epochs=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample weights\n",
    "For fine grained control, or if you are not building a classifier, you can use \"sample weights\".\n",
    "- When training from NumPy data: Pass the sample_weight argument to Model.fit().\n",
    "- When training from tf.data or any other sort of iterator: Yield (input_batch, label_batch, sample_weight_batch) tuples.\n",
    "\n",
    "A \"sample weights\" array is an array of numbers that specify how much weight each sample in a batch should have in computing the total loss. It is commonly used in imbalanced classification problems (the idea being to give more weight to rarely-seen classes).\n",
    "\n",
    "When the weights used are ones and zeros, the array can be used as a mask for the loss function (entirely discarding the contribution of certain samples to the total loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_weight = np.ones(shape=(len(y_train),))\n",
    "sample_weight[y_train == 5] = 2.0\n",
    "\n",
    "print(\"Fit with sample weight\")\n",
    "model = get_compiled_model()\n",
    "model.fit(x_train, y_train, sample_weight=sample_weight, batch_size=64, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's a matching tf.data.Dataset example:\n",
    "sample_weight = np.ones(shape=(len(y_train),))\n",
    "sample_weight[y_train == 5] = 2.0\n",
    "\n",
    "# Create a Dataset that includes sample weights\n",
    "# (3rd element in the return tuple).\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train, sample_weight))\n",
    "\n",
    "# Shuffle and slice the dataset.\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "model = get_compiled_model()\n",
    "model.fit(train_dataset, epochs=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passing data to multi-input, multi-output models\n",
    "In the previous examples, we were considering a model with a single input (a tensor of shape (764,)) and a single output (a prediction tensor of shape (10,)). But what about models that have multiple inputs or outputs?\n",
    "\n",
    "Consider the following model, which has an image input of shape (32, 32, 3) (that's (height, width, channels)) and a time series input of shape (None, 10) (that's (timesteps, features)). Our model will have two outputs computed from the combination of these inputs: a \"score\" (of shape (1,)) and a probability distribution over five classes (of shape (5,))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_input = keras.Input(shape=(32, 32, 3), name=\"img_input\")\n",
    "timeseries_input = keras.Input(shape=(None, 10), name=\"ts_input\")\n",
    "\n",
    "x1 = layers.Conv2D(3, 3)(image_input)\n",
    "x1 = layers.GlobalMaxPooling2D()(x1)\n",
    "\n",
    "x2 = layers.Conv1D(3, 3)(timeseries_input)\n",
    "x2 = layers.GlobalMaxPooling1D()(x2)\n",
    "\n",
    "x = layers.concatenate([x1, x2])\n",
    "\n",
    "score_output = layers.Dense(1, name=\"score_output\")(x)\n",
    "class_output = layers.Dense(5, name=\"class_output\")(x)\n",
    "\n",
    "model = keras.Model(\n",
    "    inputs=[image_input, timeseries_input], outputs=[score_output, class_output]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot this model, so you can clearly see what we're doing here \n",
    "# (note that the shapes shown in the plot are batch shapes, rather than per-sample shapes).\n",
    "keras.utils.plot_model(model, \"multi_input_and_output_model.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At compilation time, we can specify different losses to different outputs, by passing the loss functions as a list:\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss=[keras.losses.MeanSquaredError(), keras.losses.CategoricalCrossentropy()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we only passed a single loss function to the model, the same loss function would be applied to every output (which is not appropriate here).\n",
    "# Likewise for metrics:\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss=[keras.losses.MeanSquaredError(), keras.losses.CategoricalCrossentropy()],\n",
    "    metrics=[\n",
    "        [\n",
    "            keras.metrics.MeanAbsolutePercentageError(),\n",
    "            keras.metrics.MeanAbsoluteError(),\n",
    "        ],\n",
    "        [keras.metrics.CategoricalAccuracy()],\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we gave names to our output layers, we could also specify per-output losses and metrics via a dict:\n",
    "# We recommend the use of explicit names and dicts if you have more than 2 outputs.\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss={\n",
    "        \"score_output\": keras.losses.MeanSquaredError(),\n",
    "        \"class_output\": keras.losses.CategoricalCrossentropy(),\n",
    "    },\n",
    "    metrics={\n",
    "        \"score_output\": [\n",
    "            keras.metrics.MeanAbsolutePercentageError(),\n",
    "            keras.metrics.MeanAbsoluteError(),\n",
    "        ],\n",
    "        \"class_output\": [keras.metrics.CategoricalAccuracy()],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's possible to give different weights to different output-specific losses \n",
    "# (for instance, one might wish to privilege the \"score\" loss in our example, by giving to 2x the importance of the class loss), using the loss_weights argument:\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss={\n",
    "        \"score_output\": keras.losses.MeanSquaredError(),\n",
    "        \"class_output\": keras.losses.CategoricalCrossentropy(),\n",
    "    },\n",
    "    metrics={\n",
    "        \"score_output\": [\n",
    "            keras.metrics.MeanAbsolutePercentageError(),\n",
    "            keras.metrics.MeanAbsoluteError(),\n",
    "        ],\n",
    "        \"class_output\": [keras.metrics.CategoricalAccuracy()],\n",
    "    },\n",
    "    loss_weights={\"score_output\": 2.0, \"class_output\": 1.0},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You could also choose not to compute a loss for certain outputs, if these outputs are meant for prediction but not for training:\n",
    "\n",
    "\n",
    "# List loss version\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss=[None, keras.losses.CategoricalCrossentropy()],\n",
    ")\n",
    "\n",
    "# Or dict loss version\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss={\"class_output\": keras.losses.CategoricalCrossentropy()},\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing data to a multi-input or multi-output model in fit() works in a similar way as specifying a loss function in compile: you can pass lists of NumPy arrays (with 1:1 mapping to the outputs that received a loss function) or dicts mapping output names to NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss=[keras.losses.MeanSquaredError(), keras.losses.CategoricalCrossentropy()],\n",
    ")\n",
    "\n",
    "# Generate dummy NumPy data\n",
    "img_data = np.random.random_sample(size=(100, 32, 32, 3))\n",
    "ts_data = np.random.random_sample(size=(100, 20, 10))\n",
    "score_targets = np.random.random_sample(size=(100, 1))\n",
    "class_targets = np.random.random_sample(size=(100, 5))\n",
    "\n",
    "# Fit on lists\n",
    "model.fit([img_data, ts_data], [score_targets, class_targets], batch_size=32, epochs=1)\n",
    "\n",
    "# Alternatively, fit on dicts\n",
    "model.fit(\n",
    "    {\"img_input\": img_data, \"ts_input\": ts_data},\n",
    "    {\"score_output\": score_targets, \"class_output\": class_targets},\n",
    "    batch_size=32,\n",
    "    epochs=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's the Dataset use case: similarly as what we did for NumPy arrays, the Dataset should return a tuple of dicts.\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (\n",
    "        {\"img_input\": img_data, \"ts_input\": ts_data},\n",
    "        {\"score_output\": score_targets, \"class_output\": class_targets},\n",
    "    )\n",
    ")\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "model.fit(train_dataset, epochs=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "127b1e9fcfedd5dda8ac936388b1f24776168c431ca343c8354d3bc75daf36ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
