{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "- The Keras functional API is a way to create models that are more flexible than the tf.keras.Sequential API. \n",
    "- The functional API can handle models with non-linear topology, shared layers, and even multiple inputs or outputs.\n",
    "- The main idea is that a deep learning model is usually a directed acyclic graph (DAG) of layers. \n",
    "- So the functional API is a way to build graphs of layers.\n",
    "\n",
    "Consider the following model:  \n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "This is a basic graph with three layers. To build this model using the functional API, start by creating an input node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 784)\n",
      "<dtype: 'float32'>\n",
      "Model: \"mnist_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 784)]             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                50240     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 55,050\n",
      "Trainable params: 55,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(784,))\n",
    "# The shape of the data is set as a 784-dimensional vector. \n",
    "# The batch size is always omitted since only the shape of each sample is specified.\n",
    "# If, for example, you have an image input with a shape of (32, 32, 3), you would use:\n",
    "# Just for demonstration purposes.\n",
    "img_inputs = keras.Input(shape=(32, 32, 3))\n",
    "\n",
    "# The inputs that is returned contains information about the shape and dtype of the input data that you feed to your model. \n",
    "# Here's the shape:\n",
    "print(inputs.shape)\n",
    "# Here's the dtype\n",
    "print(inputs.dtype)\n",
    "\n",
    "# You create a new node in the graph of layers by calling a layer on this inputs object:\n",
    "dense = layers.Dense(64, activation=\"relu\")\n",
    "x = dense(inputs)\n",
    "# The \"layer call\" action is like drawing an arrow from \"inputs\" to this layer you created. \n",
    "# You're \"passing\" the inputs to the dense layer, and you get x as the output.\n",
    "# Let's add a few more layers to the graph of layers:\n",
    "x = layers.Dense(64, activation=\"relu\")(x)\n",
    "outputs = layers.Dense(10)(x)\n",
    "# At this point, you can create a Model by specifying its inputs and outputs in the graph of layers:\n",
    "model = keras.Model(inputs=inputs, outputs=outputs, name=\"mnist_model\")\n",
    "# Let's check out what the model summary looks like:\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "# You can also plot the model as a graph:\n",
    "keras.utils.plot_model(model, \"my_first_model.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "# And, optionally, display the input and output shapes of each layer in the plotted graph:\n",
    "keras.utils.plot_model(model, \"my_first_model_with_shape_info.png\", show_shapes=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training, Evaluationd and Inference\n",
    "Training, evaluation, and inference work exactly in the same way for models built using the functional API as for Sequential models.\n",
    "\n",
    "The Model class offers a built-in training loop (the fit() method) and a built-in evaluation loop (the evaluate() method). Note that you can easily customize these loops to implement training routines beyond supervised learning (e.g. GANs).\n",
    "\n",
    "Here, load the MNIST image data, reshape it into vectors, fit the model on the data (while monitoring performance on a validation split), then evaluate the model on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "750/750 [==============================] - 3s 3ms/step - loss: 0.3412 - accuracy: 0.9026 - val_loss: 0.1932 - val_accuracy: 0.9435\n",
      "Epoch 2/2\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.1617 - accuracy: 0.9523 - val_loss: 0.1437 - val_accuracy: 0.9594\n",
      "313/313 - 0s - loss: 0.1302 - accuracy: 0.9605 - 407ms/epoch - 1ms/step\n",
      "Test loss: 0.13022805750370026\n",
      "Test accuracy: 0.9605000019073486\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(60000, 784).astype(\"float32\") / 255\n",
    "x_test = x_test.reshape(10000, 784).astype(\"float32\") / 255\n",
    "\n",
    "model.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=keras.optimizers.RMSprop(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=64, epochs=2, validation_split=0.2)\n",
    "\n",
    "test_scores = model.evaluate(x_test, y_test, verbose=2)\n",
    "print(\"Test loss:\", test_scores[0])\n",
    "print(\"Test accuracy:\", test_scores[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and Serialize\n",
    "Saving the model and serialization work the same way for models built using the functional API as they do for Sequential models. The standard way to save a functional model is to call model.save() to save the entire model as a single file. You can later recreate the same model from this file, even if the code that built the model is no longer available.\n",
    "\n",
    "This saved file includes the:\n",
    "- model architecture\n",
    "- model weight values (that were learned during training)\n",
    "- model training config, if any (as passed to compile)\n",
    "- optimizer and its state, if any (to restart training where you left off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: path_to_my_model\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"path_to_my_model\")\n",
    "del model\n",
    "# Recreate the exact same model purely from the file:\n",
    "model = keras.models.load_model(\"path_to_my_model\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the same graph of layers to define multiple models\n",
    "In the functional API, models are created by specifying their inputs and outputs in a graph of layers. That means that a single graph of layers can be used to generate multiple models.\n",
    "\n",
    "In the example below, you use the same stack of layers to instantiate two models: an encoder model that turns image inputs into 16-dimensional vectors, and an end-to-end autoencoder model for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " img (InputLayer)            [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 26, 26, 16)        160       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 24, 24, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 8, 8, 32)         0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 6, 6, 32)          9248      \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 4, 4, 16)          4624      \n",
      "                                                                 \n",
      " global_max_pooling2d (Globa  (None, 16)               0         \n",
      " lMaxPooling2D)                                                  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 18,672\n",
      "Trainable params: 18,672\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"autoencoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " img (InputLayer)            [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 26, 26, 16)        160       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 24, 24, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 8, 8, 32)         0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 6, 6, 32)          9248      \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 4, 4, 16)          4624      \n",
      "                                                                 \n",
      " global_max_pooling2d (Globa  (None, 16)               0         \n",
      " lMaxPooling2D)                                                  \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 4, 4, 1)           0         \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTra  (None, 6, 6, 16)         160       \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2DT  (None, 8, 8, 32)         4640      \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " up_sampling2d (UpSampling2D  (None, 24, 24, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_transpose_2 (Conv2DT  (None, 26, 26, 16)       4624      \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_transpose_3 (Conv2DT  (None, 28, 28, 1)        145       \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 28,241\n",
      "Trainable params: 28,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_input = keras.Input(shape=(28, 28, 1), name=\"img\")\n",
    "x = layers.Conv2D(16, 3, activation=\"relu\")(encoder_input)\n",
    "x = layers.Conv2D(32, 3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(3)(x)\n",
    "x = layers.Conv2D(32, 3, activation=\"relu\")(x)\n",
    "x = layers.Conv2D(16, 3, activation=\"relu\")(x)\n",
    "encoder_output = layers.GlobalMaxPooling2D()(x)\n",
    "\n",
    "encoder = keras.Model(encoder_input, encoder_output, name=\"encoder\")\n",
    "encoder.summary()\n",
    "\n",
    "x = layers.Reshape((4, 4, 1))(encoder_output)\n",
    "x = layers.Conv2DTranspose(16, 3, activation=\"relu\")(x)\n",
    "x = layers.Conv2DTranspose(32, 3, activation=\"relu\")(x)\n",
    "x = layers.UpSampling2D(3)(x)\n",
    "x = layers.Conv2DTranspose(16, 3, activation=\"relu\")(x)\n",
    "decoder_output = layers.Conv2DTranspose(1, 3, activation=\"relu\")(x)\n",
    "\n",
    "autoencoder = keras.Model(encoder_input, decoder_output, name=\"autoencoder\")\n",
    "autoencoder.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the decoding architecture is strictly symmetrical to the encoding architecture, so the output shape is the same as the input shape (28, 28, 1).\n",
    "\n",
    "The reverse of a Conv2D layer is a Conv2DTranspose layer, and the reverse of a MaxPooling2D layer is an UpSampling2D layer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All models are callable, just like layers\n",
    "You can treat any model as if it were a layer by invoking it on an Input or on the output of another layer. By calling a model you aren't just reusing the architecture of the model, you're also reusing its weights.\n",
    "\n",
    "To see this in action, here's a different take on the autoencoder example that creates an encoder model, a decoder model, and chains them in two calls to obtain the autoencoder model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " original_img (InputLayer)   [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 26, 26, 16)        160       \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 24, 24, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 8, 8, 32)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 6, 6, 32)          9248      \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 4, 4, 16)          4624      \n",
      "                                                                 \n",
      " global_max_pooling2d_1 (Glo  (None, 16)               0         \n",
      " balMaxPooling2D)                                                \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 18,672\n",
      "Trainable params: 18,672\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoded_img (InputLayer)    [(None, 16)]              0         \n",
      "                                                                 \n",
      " reshape_1 (Reshape)         (None, 4, 4, 1)           0         \n",
      "                                                                 \n",
      " conv2d_transpose_4 (Conv2DT  (None, 6, 6, 16)         160       \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_transpose_5 (Conv2DT  (None, 8, 8, 32)         4640      \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " up_sampling2d_1 (UpSampling  (None, 24, 24, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_transpose_6 (Conv2DT  (None, 26, 26, 16)       4624      \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_transpose_7 (Conv2DT  (None, 28, 28, 1)        145       \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,569\n",
      "Trainable params: 9,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"autoencoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " img (InputLayer)            [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " encoder (Functional)        (None, 16)                18672     \n",
      "                                                                 \n",
      " decoder (Functional)        (None, 28, 28, 1)         9569      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 28,241\n",
      "Trainable params: 28,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_input = keras.Input(shape=(28, 28, 1), name=\"original_img\")\n",
    "x = layers.Conv2D(16, 3, activation=\"relu\")(encoder_input)\n",
    "x = layers.Conv2D(32, 3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(3)(x)\n",
    "x = layers.Conv2D(32, 3, activation=\"relu\")(x)\n",
    "x = layers.Conv2D(16, 3, activation=\"relu\")(x)\n",
    "encoder_output = layers.GlobalMaxPooling2D()(x)\n",
    "\n",
    "encoder = keras.Model(encoder_input, encoder_output, name=\"encoder\")\n",
    "encoder.summary()\n",
    "\n",
    "decoder_input = keras.Input(shape=(16,), name=\"encoded_img\")\n",
    "x = layers.Reshape((4, 4, 1))(decoder_input)\n",
    "x = layers.Conv2DTranspose(16, 3, activation=\"relu\")(x)\n",
    "x = layers.Conv2DTranspose(32, 3, activation=\"relu\")(x)\n",
    "x = layers.UpSampling2D(3)(x)\n",
    "x = layers.Conv2DTranspose(16, 3, activation=\"relu\")(x)\n",
    "decoder_output = layers.Conv2DTranspose(1, 3, activation=\"relu\")(x)\n",
    "\n",
    "decoder = keras.Model(decoder_input, decoder_output, name=\"decoder\")\n",
    "decoder.summary()\n",
    "\n",
    "autoencoder_input = keras.Input(shape=(28, 28, 1), name=\"img\")\n",
    "encoded_img = encoder(autoencoder_input)\n",
    "decoded_img = decoder(encoded_img)\n",
    "autoencoder = keras.Model(autoencoder_input, decoded_img, name=\"autoencoder\")\n",
    "autoencoder.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the model can be nested: a model can contain sub-models (since a model is just like a layer). A common use case for model nesting is ensembling. For example, here's how to ensemble a set of models into a single model that averages their predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    inputs = keras.Input(shape=(128,))\n",
    "    outputs = layers.Dense(1)(inputs)\n",
    "    return keras.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "model1 = get_model()\n",
    "model2 = get_model()\n",
    "model3 = get_model()\n",
    "\n",
    "inputs = keras.Input(shape=(128,))\n",
    "y1 = model1(inputs)\n",
    "y2 = model2(inputs)\n",
    "y3 = model3(inputs)\n",
    "outputs = layers.average([y1, y2, y3])\n",
    "ensemble_model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulate complex graph topologies\n",
    "### Models with multiple inputs and outputs\n",
    "The functional API makes it easy to manipulate multiple inputs and outputs. This cannot be handled with the Sequential API.\n",
    "\n",
    "For example, if you're building a system for ranking customer issue tickets by priority and routing them to the correct department, then the model will have three inputs:\n",
    "\n",
    "- the title of the ticket (text input),\n",
    "- the text body of the ticket (text input), and\n",
    "- any tags added by the user (categorical input)\n",
    "\n",
    "This model will have two outputs:\n",
    "- the priority score between 0 and 1 (scalar sigmoid output), and\n",
    "- the department that should handle the ticket (softmax output over the set of departments).\n",
    "\n",
    "You can build this model in a few lines with the functional API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tags = 12  # Number of unique issue tags\n",
    "num_words = 10000  # Size of vocabulary obtained when preprocessing text data\n",
    "num_departments = 4  # Number of departments for predictions\n",
    "\n",
    "title_input = keras.Input(\n",
    "    shape=(None,), name=\"title\"\n",
    ")  # Variable-length sequence of ints\n",
    "body_input = keras.Input(shape=(None,), name=\"body\")  # Variable-length sequence of ints\n",
    "tags_input = keras.Input(\n",
    "    shape=(num_tags,), name=\"tags\"\n",
    ")  # Binary vectors of size `num_tags`\n",
    "\n",
    "# Embed each word in the title into a 64-dimensional vector\n",
    "title_features = layers.Embedding(num_words, 64)(title_input)\n",
    "# Embed each word in the text into a 64-dimensional vector\n",
    "body_features = layers.Embedding(num_words, 64)(body_input)\n",
    "\n",
    "# Reduce sequence of embedded words in the title into a single 128-dimensional vector\n",
    "title_features = layers.LSTM(128)(title_features)\n",
    "# Reduce sequence of embedded words in the body into a single 32-dimensional vector\n",
    "body_features = layers.LSTM(32)(body_features)\n",
    "\n",
    "# Merge all available features into a single large vector via concatenation\n",
    "x = layers.concatenate([title_features, body_features, tags_input])\n",
    "\n",
    "# Stick a logistic regression for priority prediction on top of the features\n",
    "priority_pred = layers.Dense(1, name=\"priority\")(x)\n",
    "# Stick a department classifier on top of the features\n",
    "department_pred = layers.Dense(num_departments, name=\"department\")(x)\n",
    "\n",
    "# Instantiate an end-to-end model predicting both priority and department\n",
    "model = keras.Model(\n",
    "    inputs=[title_input, body_input, tags_input],\n",
    "    outputs=[priority_pred, department_pred],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "# Now plot the model:\n",
    "keras.utils.plot_model(model, \"multi_input_and_output_model.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When compiling this model, you can assign different losses to each output. You can even assign different weights to each loss -- to modulate their contribution to the total training loss.\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss=[\n",
    "        keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "        keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    ],\n",
    "    loss_weights=[1.0, 0.2],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the output layers have different names, you could also specify the losses and loss weights with the corresponding layer names:\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss={\n",
    "        \"priority\": keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "        \"department\": keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    },\n",
    "    loss_weights={\"priority\": 1.0, \"department\": 0.2},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "40/40 [==============================] - 5s 30ms/step - loss: 1.2810 - priority_loss: 0.7031 - department_loss: 2.8893\n",
      "Epoch 2/2\n",
      "40/40 [==============================] - 1s 31ms/step - loss: 1.2656 - priority_loss: 0.6998 - department_loss: 2.8291\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2a4b63166a0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model by passing lists of NumPy arrays of inputs and targets:\n",
    "# Dummy input data\n",
    "title_data = np.random.randint(num_words, size=(1280, 10))\n",
    "body_data = np.random.randint(num_words, size=(1280, 100))\n",
    "tags_data = np.random.randint(2, size=(1280, num_tags)).astype(\"float32\")\n",
    "\n",
    "# Dummy target data\n",
    "priority_targets = np.random.random(size=(1280, 1))\n",
    "dept_targets = np.random.randint(2, size=(1280, num_departments))\n",
    "\n",
    "model.fit(\n",
    "    {\"title\": title_data, \"body\": body_data, \"tags\": tags_data},\n",
    "    {\"priority\": priority_targets, \"department\": dept_targets},\n",
    "    epochs=2,\n",
    "    batch_size=32,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When calling fit with a Dataset object, it should yield either a tuple of lists like ([title_data, body_data, tags_data], [priority_targets, dept_targets]) or a tuple of dictionaries like ({'title': title_data, 'body': body_data, 'tags': tags_data}, {'priority': priority_targets, 'department': dept_targets}).\n",
    "\n",
    "### A toy ResNet model\n",
    "In addition to models with multiple inputs and outputs, the functional API makes it easy to manipulate non-linear connectivity topologies -- these are models with layers that are not connected sequentially, which the Sequential API cannot handle.\n",
    "\n",
    "A common use case for this is residual connections. Let's build a toy ResNet model for CIFAR10 to demonstrate this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"toy_resnet\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " img (InputLayer)               [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 30, 30, 32)   896         ['img[0][0]']                    \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 28, 28, 64)   18496       ['conv2d_8[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 9, 9, 64)    0           ['conv2d_9[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 9, 9, 64)     36928       ['max_pooling2d_2[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 9, 9, 64)     36928       ['conv2d_10[0][0]']              \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 9, 9, 64)     0           ['conv2d_11[0][0]',              \n",
      "                                                                  'max_pooling2d_2[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 9, 9, 64)     36928       ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 9, 9, 64)     36928       ['conv2d_12[0][0]']              \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 9, 9, 64)     0           ['conv2d_13[0][0]',              \n",
      "                                                                  'add[0][0]']                    \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 7, 7, 64)     36928       ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 64)          0           ['conv2d_14[0][0]']              \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 256)          16640       ['global_average_pooling2d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 256)          0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 10)           2570        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 223,242\n",
      "Trainable params: 223,242\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(32, 32, 3), name=\"img\")\n",
    "x = layers.Conv2D(32, 3, activation=\"relu\")(inputs)\n",
    "x = layers.Conv2D(64, 3, activation=\"relu\")(x)\n",
    "block_1_output = layers.MaxPooling2D(3)(x)\n",
    "\n",
    "x = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(block_1_output)\n",
    "x = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "block_2_output = layers.add([x, block_1_output])\n",
    "\n",
    "x = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(block_2_output)\n",
    "x = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "block_3_output = layers.add([x, block_2_output])\n",
    "\n",
    "x = layers.Conv2D(64, 3, activation=\"relu\")(block_3_output)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dense(256, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(10)(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs, name=\"toy_resnet\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "keras.utils.plot_model(model, \"mini_resnet.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170498071/170498071 [==============================] - 1719s 10us/step\n",
      "13/13 [==============================] - 3s 139ms/step - loss: 2.3365 - acc: 0.1013 - val_loss: 2.2996 - val_acc: 0.1150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2a4bd11cbb0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now train the model:\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"acc\"],\n",
    ")\n",
    "# We restrict the data to the first 1000 samples so as to limit execution time\n",
    "# on Colab. Try to train on the entire dataset until convergence!\n",
    "model.fit(x_train[:1000], y_train[:1000], batch_size=64, epochs=1, validation_split=0.2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared Layers\n",
    "Another good use for the functional API are models that use shared layers. Shared layers are layer instances that are reused multiple times in the same model -- they learn features that correspond to multiple paths in the graph-of-layers.\n",
    "\n",
    "Shared layers are often used to encode inputs from similar spaces (say, two different pieces of text that feature similar vocabulary). They enable sharing of information across these different inputs, and they make it possible to train such a model on less data. If a given word is seen in one of the inputs, that will benefit the processing of all inputs that pass through the shared layer.\n",
    "\n",
    "To share a layer in the functional API, call the same layer instance multiple times. For instance, here's an Embedding layer shared across two different text inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding for 1000 unique words mapped to 128-dimensional vectors\n",
    "shared_embedding = layers.Embedding(1000, 128)\n",
    "\n",
    "# Variable-length sequence of integers\n",
    "text_input_a = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "\n",
    "# Variable-length sequence of integers\n",
    "text_input_b = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "\n",
    "# Reuse the same layer to encode both inputs\n",
    "encoded_input_a = shared_embedding(text_input_a)\n",
    "encoded_input_b = shared_embedding(text_input_b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract and reuse nodes in the graph of layers\n",
    "Because the graph of layers you are manipulating is a static data structure, it can be accessed and inspected. And this is how you are able to plot functional models as images.\n",
    "\n",
    "This also means that you can access the activations of intermediate layers (\"nodes\" in the graph) and reuse them elsewhere -- which is very useful for something like feature extraction.\n",
    "\n",
    "Let's look at an example. This is a VGG19 model with weights pretrained on ImageNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels.h5\n",
      "574710816/574710816 [==============================] - 209s 0us/step\n"
     ]
    }
   ],
   "source": [
    "vgg19 = tf.keras.applications.VGG19()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And these are the intermediate activations of the model, obtained by querying the graph data structure:\n",
    "features_list = [layer.output for layer in vgg19.layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use these features to create a new feature-extraction model that returns the values of the intermediate layer activations:\n",
    "feat_extraction_model = keras.Model(inputs=vgg19.input, outputs=features_list)\n",
    "\n",
    "img = np.random.random((1, 224, 224, 3)).astype(\"float32\")\n",
    "extracted_features = feat_extraction_model(img)\n",
    "\n",
    "# This comes in handy for tasks like neural style transfer, among other things."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extend the API using custom layers\n",
    "tf.keras includes a wide range of built-in layers, for example:\n",
    "\n",
    "Convolutional layers: Conv1D, Conv2D, Conv3D, Conv2DTranspose\n",
    "Pooling layers: MaxPooling1D, MaxPooling2D, MaxPooling3D, AveragePooling1D\n",
    "RNN layers: GRU, LSTM, ConvLSTM2D\n",
    "BatchNormalization, Dropout, Embedding, etc.\n",
    "But if you don't find what you need, it's easy to extend the API by creating your own layers. All layers subclass the Layer class and implement:\n",
    "\n",
    "call method, that specifies the computation done by the layer.\n",
    "build method, that creates the weights of the layer (this is just a style convention since you can create weights in __init__, as well).\n",
    "To learn more about creating layers from scratch, read custom layers and models guide.\n",
    "\n",
    "The following is a basic implementation of tf.keras.layers.Dense:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDense(layers.Layer):\n",
    "    def __init__(self, units=32):\n",
    "        super(CustomDense, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(\n",
    "            shape=(input_shape[-1], self.units),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            shape=(self.units,), initializer=\"random_normal\", trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "\n",
    "\n",
    "inputs = keras.Input((4,))\n",
    "outputs = CustomDense(10)(inputs)\n",
    "\n",
    "model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For serialization support in your custom layer, define a get_config method that returns the constructor arguments of the layer instance:\n",
    "class CustomDense(layers.Layer):\n",
    "    def __init__(self, units=32):\n",
    "        super(CustomDense, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(\n",
    "            shape=(input_shape[-1], self.units),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            shape=(self.units,), initializer=\"random_normal\", trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\"units\": self.units}\n",
    "\n",
    "\n",
    "inputs = keras.Input((4,))\n",
    "outputs = CustomDense(10)(inputs)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "config = model.get_config()\n",
    "\n",
    "new_model = keras.Model.from_config(config, custom_objects={\"CustomDense\": CustomDense})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, implement the class method from_config(cls, config) which is used when recreating a layer instance given its config dictionary. The default implementation of from_config is:\n",
    "def from_config(cls, config):\n",
    "  return cls(**config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to use the functional API\n",
    "Should you use the Keras functional API to create a new model, or just subclass the Model class directly? In general, the functional API is higher-level, easier and safer, and has a number of features that subclassed models do not support.\n",
    "\n",
    "However, model subclassing provides greater flexibility when building models that are not easily expressible as directed acyclic graphs of layers. For example, you could not implement a Tree-RNN with the functional API and would have to subclass Model directly.\n",
    "\n",
    "For an in-depth look at the differences between the functional API and model subclassing, read What are Symbolic and Imperative APIs in TensorFlow 2.0?.\n",
    "\n",
    "### Functional API strengths:\n",
    "The following properties are also true for Sequential models (which are also data structures), but are not true for subclassed models (which are Python bytecode, not data structures).\n",
    "\n",
    "#### Less verbose\n",
    "There is no super(MyClass, self).__init__(...), no def call(self, ...):, etc.\n",
    "\n",
    "Compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(32,))\n",
    "x = layers.Dense(64, activation='relu')(inputs)\n",
    "outputs = layers.Dense(10)(x)\n",
    "mlp = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the subclassed version:\n",
    "class MLP(keras.Model):\n",
    "\n",
    "  def __init__(self, **kwargs):\n",
    "    super(MLP, self).__init__(**kwargs)\n",
    "    self.dense_1 = layers.Dense(64, activation='relu')\n",
    "    self.dense_2 = layers.Dense(10)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    x = self.dense_1(inputs)\n",
    "    return self.dense_2(x)\n",
    "\n",
    "# Instantiate the model.\n",
    "mlp = MLP()\n",
    "# Necessary to create the model's state.\n",
    "# The model doesn't have a state until it's called at least once.\n",
    "_ = mlp(tf.zeros((1, 32)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model validation while defining its connectivity graph\n",
    "In the functional API, the input specification (shape and dtype) is created in advance (using Input). Every time you call a layer, the layer checks that the specification passed to it matches its assumptions, and it will raise a helpful error message if not.\n",
    "\n",
    "This guarantees that any model you can build with the functional API will run. All debugging -- other than convergence-related debugging -- happens statically during the model construction and not at execution time. This is similar to type checking in a compiler.\n",
    "\n",
    "#### A functional model is plottable and inspectable\n",
    "You can plot the model as a graph, and you can easily access intermediate nodes in this graph. For example, to extract and reuse the activations of intermediate layers (as seen in a previous example):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list = [layer.output for layer in vgg19.layers]\n",
    "feat_extraction_model = keras.Model(inputs=vgg19.input, outputs=features_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A functional model can be serialized or cloned\n",
    "Because a functional model is a data structure rather than a piece of code, it is safely serializable and can be saved as a single file that allows you to recreate the exact same model without having access to any of the original code. See the serialization & saving guide.\n",
    "\n",
    "To serialize a subclassed model, it is necessary for the implementer to specify a get_config() and from_config() method at the model level.\n",
    "\n",
    "### Functional API weakness:\n",
    "#### It does not support dynamic architectures\n",
    "The functional API treats models as DAGs of layers. This is true for most deep learning architectures, but not all -- for example, recursive networks or Tree RNNs do not follow this assumption and cannot be implemented in the functional API.\n",
    "\n",
    "### Mix-and-match API styles\n",
    "Choosing between the functional API or Model subclassing isn't a binary decision that restricts you into one category of models. All models in the tf.keras API can interact with each other, whether they're Sequential models, functional models, or subclassed models that are written from scratch.\n",
    "\n",
    "You can always use a functional model or Sequential model as part of a subclassed model or layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 10, 32)\n"
     ]
    }
   ],
   "source": [
    "units = 32\n",
    "timesteps = 10\n",
    "input_dim = 5\n",
    "\n",
    "# Define a Functional model\n",
    "inputs = keras.Input((None, units))\n",
    "x = layers.GlobalAveragePooling1D()(inputs)\n",
    "outputs = layers.Dense(1)(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "class CustomRNN(layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(CustomRNN, self).__init__()\n",
    "        self.units = units\n",
    "        self.projection_1 = layers.Dense(units=units, activation=\"tanh\")\n",
    "        self.projection_2 = layers.Dense(units=units, activation=\"tanh\")\n",
    "        # Our previously-defined Functional model\n",
    "        self.classifier = model\n",
    "\n",
    "    def call(self, inputs):\n",
    "        outputs = []\n",
    "        state = tf.zeros(shape=(inputs.shape[0], self.units))\n",
    "        for t in range(inputs.shape[1]):\n",
    "            x = inputs[:, t, :]\n",
    "            h = self.projection_1(x)\n",
    "            y = h + self.projection_2(state)\n",
    "            state = y\n",
    "            outputs.append(y)\n",
    "        features = tf.stack(outputs, axis=1)\n",
    "        print(features.shape)\n",
    "        return self.classifier(features)\n",
    "\n",
    "\n",
    "rnn_model = CustomRNN()\n",
    "_ = rnn_model(tf.zeros((1, timesteps, input_dim)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use any subclassed layer or model in the functional API as long as it implements a call method that follows one of the following patterns:\n",
    "\n",
    "- call(self, inputs, **kwargs) -- Where inputs is a tensor or a nested structure of tensors (e.g. a list of tensors), and where **kwargs are non-tensor arguments (non-inputs).\n",
    "- call(self, inputs, training=None, **kwargs) -- Where training is a boolean indicating whether the layer should behave in training mode and inference mode.\n",
    "- call(self, inputs, mask=None, **kwargs) -- Where mask is a boolean mask tensor (useful for RNNs, for instance).\n",
    "- call(self, inputs, training=None, mask=None, **kwargs) -- Of course, you can have both masking and training-specific behavior at the same time.\n",
    "\n",
    "Additionally, if you implement the get_config method on your custom Layer or model, the functional models you create will still be serializable and cloneable.\n",
    "\n",
    "Here's a quick example of a custom RNN, written from scratch, being used in a functional model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "units = 32\n",
    "timesteps = 10\n",
    "input_dim = 5\n",
    "batch_size = 16\n",
    "\n",
    "\n",
    "class CustomRNN(layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(CustomRNN, self).__init__()\n",
    "        self.units = units\n",
    "        self.projection_1 = layers.Dense(units=units, activation=\"tanh\")\n",
    "        self.projection_2 = layers.Dense(units=units, activation=\"tanh\")\n",
    "        self.classifier = layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        outputs = []\n",
    "        state = tf.zeros(shape=(inputs.shape[0], self.units))\n",
    "        for t in range(inputs.shape[1]):\n",
    "            x = inputs[:, t, :]\n",
    "            h = self.projection_1(x)\n",
    "            y = h + self.projection_2(state)\n",
    "            state = y\n",
    "            outputs.append(y)\n",
    "        features = tf.stack(outputs, axis=1)\n",
    "        return self.classifier(features)\n",
    "\n",
    "\n",
    "# Note that you specify a static batch size for the inputs with the `batch_shape`\n",
    "# arg, because the inner computation of `CustomRNN` requires a static batch size\n",
    "# (when you create the `state` zeros tensor).\n",
    "inputs = keras.Input(batch_shape=(batch_size, timesteps, input_dim))\n",
    "x = layers.Conv1D(32, 3)(inputs)\n",
    "outputs = CustomRNN()(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "rnn_model = CustomRNN()\n",
    "_ = rnn_model(tf.zeros((1, 10, 5)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_PT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
