{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np, pandas as pd\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Layer Class\n",
    "- tf.keras.layers.Layer class is an abstract class which will be used define custom layers\n",
    "- This class have both state (layer weights) and and a transformation from inputs to outputs\n",
    "- Lets see how we can use Layer class to define a Dense layer with state: the variables w and b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.06529395  0.02208516 -0.00957388 -0.00714765]\n",
      " [ 0.06529395  0.02208516 -0.00957388 -0.00714765]], shape=(2, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "class Linear(tf.keras.layers.Layer):\n",
    "    def __init__(self, units=32, input_dim=32):\n",
    "        super(Linear, self).__init__()\n",
    "        w_init = tf.random_normal_initializer()\n",
    "        self.w = tf.Variable(initial_value=w_init(shape=(input_dim, units), dtype=\"float32\"),\n",
    "                             trainable=True)\n",
    "        b_init = tf.zeros_initializer()\n",
    "        self.b = tf.Variable(initial_value=b_init(shape=(units,), dtype=\"float32\"), trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "    \n",
    "# You would use a layer by calling it on some tensor input(s), much like a Python function.\n",
    "\n",
    "x = tf.ones((2,2))\n",
    "linear_layer = Linear(4,2)\n",
    "y = linear_layer(x)\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights w and b are automatically tracked by the layer upon being set as layer attributes:\n",
    "assert linear_layer.weights == [linear_layer.w, linear_layer.b] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.0175115  -0.10254346  0.06433796  0.03940462]\n",
      " [-0.0175115  -0.10254346  0.06433796  0.03940462]], shape=(2, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Instead of initializing the values and assign them to a variable we can use add_weight method as shown below\n",
    "class Linear(tf.keras.layers.Layer):\n",
    "    def __init__(self, units=32, input_dim=32):\n",
    "        super(Linear, self).__init__()\n",
    "        self.w = self.add_weight(shape=(input_dim, units), initializer = \"random_normal\", trainable=True)\n",
    "        self.b = self.add_weight(shape=(units,), initializer = \"zeros\", trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "    \n",
    "# You would use a layer by calling it on some tensor input(s), much like a Python function.\n",
    "\n",
    "x = tf.ones((2,2))\n",
    "linear_layer = Linear(4,2)\n",
    "y = linear_layer(x)\n",
    "print(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers can have non-trainable weights\n",
    "- Besides trainable weights, you can add non-trainable weights to a layer as well. \n",
    "- Such weights are meant not to be taken into account during backpropagation, when you are training the layer.\n",
    "\n",
    "Here's how to add and use a non-trainable weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2.]\n",
      "[4. 4.]\n"
     ]
    }
   ],
   "source": [
    "class ComputeSum(tf.keras.layers.Layer):\n",
    "    def __init__(self, input_dim):\n",
    "        super(ComputeSum, self).__init__()\n",
    "        self.total = tf.Variable(initial_value = tf.zeros((input_dim,)), trainable=False)\n",
    "    def call(self, inputs):\n",
    "        self.total.assign_add(tf.reduce_sum(inputs, axis=0))\n",
    "        return self.total\n",
    "    \n",
    "x = tf.ones((2, 2))\n",
    "my_sum = ComputeSum(2)\n",
    "y = my_sum(x)\n",
    "print(y.numpy())\n",
    "y = my_sum(x)\n",
    "print(y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: 1\n",
      "non-trainable weights: 1\n",
      "trainable_weights: []\n"
     ]
    }
   ],
   "source": [
    "# It's part of layer.weights, but it gets categorized as a non-trainable weight:\n",
    "\n",
    "\n",
    "print(\"weights:\", len(my_sum.weights))\n",
    "print(\"non-trainable weights:\", len(my_sum.non_trainable_weights))\n",
    "\n",
    "# It's not included in the trainable weights:\n",
    "print(\"trainable_weights:\", my_sum.trainable_weights)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practice:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_PT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
