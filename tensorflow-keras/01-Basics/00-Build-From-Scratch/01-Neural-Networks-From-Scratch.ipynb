{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network implementation from scratch using python\n",
    "\n",
    "Hi Folks lets learn implementing neural networks from scratch using pure python and not using any ML modules.\n",
    "I am assuming that we have a bit knowledge in python and ML and implementing the Neural Networks based on it.\n",
    "\n",
    "This Notebook consists of following modules:\n",
    "- Neural Netwrok Layers\n",
    "- weight initialization\n",
    "- Model Development\n",
    "- Gradient Descent and Weight updation\n",
    "- Training the algorithm for multiple epoches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, sys\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "plt.rcParams['figure.figsize']=(20,20)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions\n",
    "Activation functions are generally used for adding non-linearity to the function and there are so many non-linear functions and a few of useful non-linear functions are\n",
    "- sigmoid\n",
    "- tanh\n",
    "- ReLU\n",
    "- leaky ReLU\n",
    "- softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class activationFunctions:\n",
    "    def __init__(self, input:np.ndarray, activation='linear'):\n",
    "        self.activation = activation\n",
    "        self.input = input\n",
    "        self.result = self.selectActivation(activation)\n",
    "    \n",
    "    def selectActivation(self):\n",
    "        switcher = {\n",
    "            'linear' : input,\n",
    "            'sigmoid' : self.sigmoid(input),\n",
    "            'tanh' : self.tanH(input),\n",
    "            'relu' : self.relu(input),\n",
    "            'leakyRelu' : self.leakyRelu(input),\n",
    "            'softmax': self.softmax(input)\n",
    "        }\n",
    "        return switcher.get(self.activation, 'notDefined')\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(self) -> np.ndarray:\n",
    "        return (1.0)/(1+np.exp(-self.input))\n",
    "    \n",
    "    @staticmethod\n",
    "    def tanH(self) -> np.ndarray:\n",
    "        return (np.exp(self.input)-np.exp(-self.input))/(np.exp(self.input)+np.exp(-self.input))\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu(self) -> np.ndarray:\n",
    "        return np.max(0,self.input)\n",
    "\n",
    "    @staticmethod\n",
    "    def leakyRelu(self) -> np.ndarray:\n",
    "        return np.where(self.input>0, self.input, self.input*0.01)\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax(self) -> np.ndarray:\n",
    "        return np.exp(self.input)/(np.sum(np.exp(self.input), axis=0))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Normalization and standardization\n",
    "- Normalization (x-min)/(max-min)\n",
    "- Standardization (x-mue)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataNormalization:\n",
    "    def __init__(self, input:np.ndarray, standard:bool=False)->np.ndarray:\n",
    "        self.input = input\n",
    "        if ~standard:\n",
    "            self.result =  self.normalize()\n",
    "        else:\n",
    "            self.result =  self.standardize()\n",
    "    \n",
    "    def normalize(self)-> np.ndarray:\n",
    "        self.max = np.max(self.input)\n",
    "        self.min = np.min(self.input)\n",
    "        return (self.input-self.min)/(self.max - self.min)\n",
    "\n",
    "    def standardise(self) -> np.ndarray:\n",
    "        self.mean = np.mean(self.input)\n",
    "        self.std = np.std(self.input)\n",
    "        return (self.input - self.mean)/(self.std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding the outputs\n",
    "In case of classification output is categoriical which needs a special format one hot encoding is best suitable format for that. One hot encoding results the binary code with only one bit highlighted corresponding to class number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(input:np.ndarray, num_of_labels:int) -> np.ndarray:\n",
    "    return np.eye(num_of_labels)[input]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivative Functions\n",
    "This section deals with the derivatives of some of the activation function which eases our implemntation \n",
    "Follwing results are derivative results with respect to input\n",
    "- Derivative of sigmoid = sigmoid * (1-sigmoid)\n",
    "- Derivative of tanh = 1- tanh2\n",
    "- Derivative of relu = 1 when input>0 else 0\n",
    "- Derivative of leaky relu = 1 when input>0 else 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative(activation:str, input:np.ndarray) -> np.ndarray:\n",
    "    act = activationFunctions(input, activation=activation)\n",
    "    \n",
    "    if activation==\"sigmoid\":\n",
    "        return  act.result * (1- act.result)\n",
    "    elif activation==\"tanh\":\n",
    "        return 1 - np.square(act.result)\n",
    "    elif activation == \"relu\":\n",
    "        return (input>0)*1\n",
    "    elif activation==\"leakyRelu\":\n",
    "        return np.where(input>0, 1, 0.01)\n",
    "    elif activation==\"linear\":\n",
    "        return input\n",
    "    else:\n",
    "        return \"no such activation\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Netwrok Implemntation\n",
    "So far we developed all the required functions now we use all those along with some more mathematical implementations of the network\n",
    "\n",
    "Usually any neural network involves three steps\n",
    "- Feed Forward Propogation\n",
    "- Cost estimation\n",
    "- Back Prpogation\n",
    "\n",
    "After these we will estimate the model performance using accuracy metrics according the problem definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict()\n",
    "config['hiddenLayers'] = {\n",
    "    1:{'units':128, 'activation':'relu'},\n",
    "    2:{'units':64, 'activation':'relu'}\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class createNN:\n",
    "    def __init__(self, input:np.ndarray, output:np.ndarray, config:dict, classification=False):\n",
    "        self.config = config\n",
    "        self.classification = classification\n",
    "        self.input = input\n",
    "        self.norm = dataNormalization(self.input)\n",
    "        self.X = self.noram.result\n",
    "        assert np.all((self.X>=0) & (self.X<=1)) # test whether normalization worked on not\n",
    "        self.y = output\n",
    "        if self.classification:\n",
    "            self.y = one_hot_encode(output, np.unique(output))\n",
    "        self.inputVectorDims = self.X.shape[0]\n",
    "        self.noOfSamples = self.X.shape[1]\n",
    "        self.layers = {}\n",
    "        self.parameters = {}\n",
    "        self.L = len(config['hiddenLayers'])+1\n",
    "        self.architecture = self.config['hiddenLayers']\n",
    "        self.architecture[0] = {'units':self.inputVectorDims,'activation':'linear'}\n",
    "        self.architecture[len(self.architecture)] = {'units':len(np.unique(output)), 'activation': 'softmax' if classification else 'linear'}\n",
    "        \n",
    "    def initializeParamters(self, method=None):\n",
    "        # follow this link for more information on Weight Initialization\n",
    "        # https://machinelearningmastery.com/weight-initialization-for-deep-learning-neural-networks/\n",
    "        for i in range(1, self.L+1):\n",
    "            print(f\"Initializing the parameters for the layer {i}...\")\n",
    "            self.parameters[f'w{i}'] = np.random.randn(self.config[i]['units'], self.config[i-1]['units'])\n",
    "            self.parameters[f'b{i}'] = np.zeros(self.architecture[i]['units'], 1)\n",
    "    \n",
    "    def forwardPropogation(self):\n",
    "        params = self.parameters\n",
    "        self.layers[\"a0\"] = self.X\n",
    "        for l in range(1, self.L+1):\n",
    "            self.layers[f'z{l}'] = np.dot(params[f'w{l}'], self.layers[f'a{l-1}']) + params[f'b{l}']\n",
    "            self.layers[f'a{l}'] = activationFunctions(self.layers[f'z{l}'], self.architecture[l]['activation']).result\n",
    "        \n",
    "        self.output = self.layers[f'a{self.L}']\n",
    "        self.cost = self.estimateError()\n",
    "\n",
    "    def estimateError(self):\n",
    "        if self.classification:\n",
    "            return -np.sum(self.output * np.log(self.output + 0.000000001)) \n",
    "            # added 0.00000001 to avoid divisionn by zeros\n",
    "        else:\n",
    "            return np.square(self.predictions-self.output)\n",
    "\n",
    "    def backPropogation(self):\n",
    "        # derivative of cross entropy loss wrt softmax is (preidtcions - labels)\n",
    "        # please refer https://deepnotes.io/softmax-crossentropy#:~:text=Derivative%20of%20Softmax,-Due%20to%20the&text=In%20our%20case%20%5C(g,%5C(e%5E%7Ba_j%7D%5C).\n",
    "        derivatives = {}\n",
    "        if self.classification:\n",
    "            dZ = self.output - self.y\n",
    "        else:\n",
    "            da = -(2/self.noOfSamples)*(np.sum(self.y-self.output))\n",
    "            daz = derivative(self.architecture[self.L]['activation'], self.layers[f'a{self.L}'])\n",
    "            dZ = da*daz\n",
    "        dW = np.dot(dZ, self.layers[f'a{self.L-2}'].T)/self.noOfSamples\n",
    "        db = np.sum(dZ, axis=1, keepdims=True)/self.noOfSamples\n",
    "        dAPrev = np.dot(self.parameters[f'w{self.L-1}'].T, dZ)\n",
    "        derivatives[f'dW{self.L-1}'] = dW\n",
    "        derivatives[f'db{self.L-1}'] = db\n",
    "\n",
    "        for l in range(self.L-1, 0, -1):\n",
    "            dZ = dAPrev * derivative(self.architecture[l+1]['activation'], self.layers[f'z{l}'])\n",
    "            dW = (1./self.noOfSamples) * (np.dot(dZ, self.layers[f'a{l-1}'].T))\n",
    "            db = (1./self.noOfSamples) * (np.sum(dZ, axis=1, keepdims=True))\n",
    "            if l>1:\n",
    "                dAPrev = np.dot(self.parameters[f'w{l}'].T, dZ)\n",
    "            derivatives[f'dW{l}'] = dW\n",
    "            derivatives[f'db{l}'] = db\n",
    "\n",
    "        self.derivatives = derivative\n",
    "\n",
    "    def fit(self, lr=0.01, epochs=1000):\n",
    "        self.costs = []\n",
    "        self.initializeParamters()\n",
    "        self.accuracies = {'train': [], 'val':[]}\n",
    "        for epoch in tqdm(range(epochs), color='BLUE'):\n",
    "            cost, cache = self.forwardPropogation()\n",
    "            self.costs.append(self.cost)\n",
    "            self.backPropogation()\n",
    "            for layer in range(1, self.L+1):\n",
    "                self.parameters[\"w\"+str(layer)] = self.parameters[\"w\"+str(layer)] - lr * self.derivatives[\"dW\" + str(layer)]\n",
    "                self.parameters[\"b\"+str(layer)] = self.parameters[\"b\"+str(layer)] - lr * self.derivatives[\"db\" + str(layer)]  \n",
    "\n",
    "            train_accuracy = self.accuracy(self.X, self.y)\n",
    "            val_accuracy = self.accuracy(self.X_val, self.y_val)\n",
    "\n",
    "            if epoch %10 ==0:\n",
    "                print(f\"Epoch: {epoch:3d} | Cost: {cost:.3f} | Accuracy: {train_accuracy:.3f}\")\n",
    "\n",
    "            self.accuracies[\"train\"].append(train_accuracy)\n",
    "            self.accuracies[\"val\"].append(val_accuracy)\n",
    "\n",
    "    def accuracy(self, X, y):\n",
    "        P = self.predict(X)\n",
    "        return sum(np.equal(P, np.argmax(y, axis=0))) / y.shape[1]*100\n",
    "    \n",
    "    def predict(self, X):\n",
    "        params = self.parameters\n",
    "        values = [X]\n",
    "        for l in range(1, self.L):\n",
    "            z = np.dot(params[f\"w{l}\"], values[l-1]) + params[f'b{l}']\n",
    "            a = eval(self.architecture[l]['activation'])(z)\n",
    "            values.append(a)\n",
    "        if X.shape[1]>1:\n",
    "            ans = np.argmax(a, axis=0)\n",
    "        else:\n",
    "            ans = np.argmax(a)\n",
    "        \n",
    "        return ans\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'target', 'frame', 'categories', 'feature_names', 'target_names', 'DESCR', 'details', 'url'])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml(name=\"mnist_784\")\n",
    "\n",
    "print(mnist.keys())\n",
    "\n",
    "data = mnist.data\n",
    "labels = mnist.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67215\n",
      "(784,)\n",
      "3\n",
      "(784,)\n",
      "Image label: 3\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = np.random.choice(np.arange(data.shape[0]+1))\n",
    "\n",
    "print(n)\n",
    "\n",
    "test_img = data.iloc[n].values\n",
    "print(test_img.shape)\n",
    "test_label = mnist.target.iloc[n]\n",
    "print(test_label)\n",
    "\n",
    "print(test_img.shape)\n",
    "\n",
    "side_length = int(np.sqrt(test_img.shape))\n",
    "reshaped_test_img = test_img.reshape(side_length, side_length)\n",
    "\n",
    "print(\"Image label: \" + str(test_label))\n",
    "\n",
    "plt.imshow(reshaped_test_img, cmap=\"Greys\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 60000) (784, 10000)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'NN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-b339db7d7cbf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mPARAMS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"relu\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mnn_relu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mPARAMS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mepochs_relu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mlr_relu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.003\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'NN' is not defined"
     ]
    }
   ],
   "source": [
    "train_test_split_no = 60000\n",
    "X_train = data.values[:train_test_split_no].T\n",
    "y_train = labels[:train_test_split_no].values.astype(int)\n",
    "X_test = data.values[train_test_split_no:].T\n",
    "y_test = labels[train_test_split_no:].values.astype(int)\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "\n",
    "PARAMS = [X_train, y_train, X_test, y_test, \"relu\", 10, [128, 32]]\n",
    "nn_relu = NN(*PARAMS)\n",
    "epochs_relu = 200\n",
    "lr_relu = 0.003\n",
    "nn_relu.fit(X_train, y_train, lr=lr_relu, epochs=epochs_relu)\n",
    "nn_relu.plot_cost(lr_relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('DL_TF')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "948d505e6a6d7f649fbd0db86d942010ad20f6083c1676336ac410c28aeae8e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
