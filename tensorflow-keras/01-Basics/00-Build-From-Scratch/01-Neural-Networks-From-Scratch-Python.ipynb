{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network implementation from scratch using python\n",
    "\n",
    "Hi Folks lets learn implementing neural networks from scratch using pure python and not using any ML modules.\n",
    "I am assuming that we have a bit knowledge in python and ML and implementing the Neural Networks based on it.\n",
    "\n",
    "This Notebook consists of following modules:\n",
    "- Neural Netwrok Layers\n",
    "- weight initialization\n",
    "- Model Development\n",
    "- Gradient Descent and Weight updation\n",
    "- Training the algorithm for multiple epoches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, sys\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from typing import List, Tuple\n",
    "import copy\n",
    "plt.rcParams['figure.figsize']=(20,20)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions\n",
    "Activation functions are generally used for adding non-linearity to the function and there are so many non-linear functions and a few of useful non-linear functions are\n",
    "- sigmoid\n",
    "- tanh\n",
    "- ReLU\n",
    "- leaky ReLU\n",
    "- softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class activationFunctions:\n",
    "    def __init__(self, input:np.ndarray, activation='linear'):\n",
    "        self.activation = activation\n",
    "        self.input = input\n",
    "        self.result = self.selectActivation()\n",
    "    \n",
    "    def selectActivation(self):\n",
    "        switcher = {\n",
    "            'linear' : self.input,\n",
    "            'sigmoid' : self.sigmoid(),\n",
    "            'tanh' : self.tanH(),\n",
    "            'relu' : self.relu(),\n",
    "            'leakyRelu' : self.leakyRelu(),\n",
    "            'softmax': self.softmax()\n",
    "        }\n",
    "        return switcher.get(self.activation, 'notDefined')\n",
    "\n",
    "    def sigmoid(self) -> np.ndarray:\n",
    "        return (1.0)/(1.0 +np.exp(-self.input))\n",
    "    \n",
    "    def tanH(self) -> np.ndarray:\n",
    "        return (np.exp(self.input)-np.exp(-self.input))/(np.exp(self.input)+np.exp(-self.input))\n",
    "\n",
    "    def relu(self) -> np.ndarray:\n",
    "        return np.maximum(0, self.input)\n",
    "\n",
    "    def leakyRelu(self) -> np.ndarray:\n",
    "        return np.where(self.input>0, self.input, self.input*0.01)\n",
    "\n",
    "    def softmax(self) -> np.ndarray:\n",
    "        e = np.exp(self.input - np.max(self.input))\n",
    "        return e/np.sum(e, axis=0, keepdims=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Normalization and standardization\n",
    "- Normalization (x-min)/(max-min)\n",
    "- Standardization (x-mue)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataNormalization:\n",
    "    def __init__(self, input:np.ndarray, standard:bool=False)->np.ndarray:\n",
    "        self.input = input\n",
    "        if standard:\n",
    "            self.result =  self.standardise()\n",
    "        else:\n",
    "            self.result =  self.normalize()\n",
    "    \n",
    "    def normalize(self)-> np.ndarray:\n",
    "        self.max = np.max(self.input)\n",
    "        self.min = np.min(self.input)\n",
    "        return (self.input-self.min)/(self.max - self.min)\n",
    "\n",
    "    def standardise(self) -> np.ndarray:\n",
    "        self.mean = np.mean(self.input)\n",
    "        self.std = np.std(self.input)\n",
    "        return (self.input - self.mean)/(self.std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding the outputs\n",
    "In case of classification output is categoriical which needs a special format one hot encoding is best suitable format for that. One hot encoding results the binary code with only one bit highlighted corresponding to class number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(input:np.ndarray, num_of_labels:int) -> np.ndarray:\n",
    "    return np.eye(num_of_labels)[input]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivative Functions\n",
    "This section deals with the derivatives of some of the activation function which eases our implemntation \n",
    "Follwing results are derivative results with respect to input\n",
    "- Derivative of sigmoid = sigmoid * (1-sigmoid)\n",
    "- Derivative of tanh = 1- tanh2\n",
    "- Derivative of relu = 1 when input>0 else 0\n",
    "- Derivative of leaky relu = 1 when input>0 else 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative(activation:str, input:np.ndarray) -> np.ndarray:\n",
    "    act = activationFunctions(input, activation=activation)\n",
    "    \n",
    "    if activation==\"sigmoid\":\n",
    "        return  act.result * (1- act.result)\n",
    "    elif activation==\"tanh\":\n",
    "        return 1 - np.square(act.result)\n",
    "    elif activation == \"relu\":\n",
    "        return (input>0)*1\n",
    "    elif activation==\"leakyRelu\":\n",
    "        return np.where(input>0, 1, 0.01)\n",
    "    elif activation==\"linear\":\n",
    "        return input\n",
    "    else:\n",
    "        return \"no such activation\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Netwrok Implemntation\n",
    "So far we developed all the required functions now we use all those along with some more mathematical implementations of the network\n",
    "\n",
    "Usually any neural network involves three steps\n",
    "- Feed Forward Propogation\n",
    "- Cost estimation\n",
    "- Back Prpogation\n",
    "\n",
    "After these we will estimate the model performance using accuracy metrics according the problem definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict()\n",
    "config['noOfLabels'] = 10\n",
    "config['hiddenLayers'] = {\n",
    "    1:{'units':512, 'activation':'sigmoid'},\n",
    "    2:{'units':300, 'activation':'sigmoid'}\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'target', 'frame', 'categories', 'feature_names', 'target_names', 'DESCR', 'details', 'url'])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml(name=\"mnist_784\")\n",
    "\n",
    "print(mnist.keys())\n",
    "\n",
    "data = mnist.data\n",
    "labels = mnist.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48607\n",
      "(784,)\n",
      "0\n",
      "(784,)\n",
      "Image label: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = np.random.choice(np.arange(data.shape[0]+1))\n",
    "\n",
    "print(n)\n",
    "\n",
    "test_img = data.iloc[n].values\n",
    "print(test_img.shape)\n",
    "test_label = mnist.target.iloc[n]\n",
    "print(test_label)\n",
    "\n",
    "print(test_img.shape)\n",
    "\n",
    "side_length = int(np.sqrt(test_img.shape))\n",
    "reshaped_test_img = test_img.reshape(side_length, side_length)\n",
    "\n",
    "print(\"Image label: \" + str(test_label))\n",
    "\n",
    "plt.imshow(reshaped_test_img, cmap=\"Greys\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class createNN:\n",
    "    def __init__(self, input:np.ndarray, output:np.ndarray, config:dict=None, classification=False, validation:List[np.ndarray] = None):\n",
    "        self.config = copy.deepcopy(config)\n",
    "        self.classification = classification\n",
    "        self.input = input.copy()\n",
    "        self.noOfLabels = self.config['noOfLabels']\n",
    "        self.validation = validation\n",
    "        self.norm = dataNormalization(self.input)\n",
    "        self.X = self.norm.result\n",
    "        assert np.all((self.X>=0) & (self.X<=1)) # test whether normalization worked on not\n",
    "        self.y = output\n",
    "        if self.classification:\n",
    "            self.y = one_hot_encode(output, self.noOfLabels).T\n",
    "        if validation is not None:\n",
    "            self.X_val = (validation[0]-self.norm.min)/(self.norm.max - self.norm.min)\n",
    "            self.y_val = validation[1]\n",
    "            if self.classification:\n",
    "                self.y_val = one_hot_encode(self.y_val, self.noOfLabels).T\n",
    "        self.inputVectorDims = self.X.shape[0]\n",
    "        self.noOfSamples = self.X.shape[1]\n",
    "        self.layers = {}\n",
    "        self.parameters = {}\n",
    "        self.L = len(config['hiddenLayers'])+1\n",
    "        self.architecture = copy.deepcopy(config['hiddenLayers'])\n",
    "        self.architecture[0] = {'units':self.inputVectorDims,'activation':'linear'}\n",
    "        self.architecture[self.L] = {'units':self.noOfLabels, 'activation': 'softmax' if classification else 'linear'}\n",
    "        \n",
    "    def initializeParamters(self, method=None):\n",
    "        # follow this link for more information on Weight Initialization\n",
    "        # https://machinelearningmastery.com/weight-initialization-for-deep-learning-neural-networks/\n",
    "        for i in range(1, self.L+1):\n",
    "            print(f\"Initializing the parameters for the layer {i}...\")\n",
    "            self.parameters[f'w{i}'] = np.random.randn(self.architecture[i]['units'], self.architecture[i-1]['units']) *0.01\n",
    "            self.parameters[f'b{i}'] = np.zeros((self.architecture[i]['units'], 1))\n",
    "        print(f\"Pramaters and Lengths: {[(k, obj.shape) for k,obj in self.parameters.items()]}\")\n",
    "    \n",
    "    def forwardPropogation(self):\n",
    "        params = self.parameters\n",
    "        self.layers[\"a0\"] = self.X\n",
    "        for l in range(1, self.L+1):\n",
    "            self.layers[f'z{l}'] = np.dot(params[f'w{l}'], self.layers[f'a{l-1}']) + params[f'b{l}']\n",
    "            self.layers[f'a{l}'] = activationFunctions(self.layers[f'z{l}'], self.architecture[l]['activation']).result\n",
    "        \n",
    "        assert self.layers[f'a{self.L}'].shape == (self.noOfLabels, self.noOfSamples)\n",
    "        assert all([s for s in np.sum(self.layers[f'a{self.L}'], axis=1)])    \n",
    "        \n",
    "        self.output = self.layers[f'a{self.L}']\n",
    "        self.cost = self.estimateError()\n",
    "\n",
    "    def estimateError(self):\n",
    "        if self.classification:\n",
    "            return -np.sum(self.y * np.log(self.output + 0.000000001)) \n",
    "            # added 0.00000001 to avoid divisionn by zeros\n",
    "        else:\n",
    "            return np.square(self.predictions-self.output)\n",
    "\n",
    "    def backPropogation(self):\n",
    "        # derivative of cross entropy loss wrt softmax is (preidtcions - labels)\n",
    "        # please refer https://deepnotes.io/softmax-crossentropy#:~:text=Derivative%20of%20Softmax,-Due%20to%20the&text=In%20our%20case%20%5C(g,%5C(e%5E%7Ba_j%7D%5C).\n",
    "        derivatives = {}\n",
    "        if self.classification:\n",
    "            dZ = self.output - self.y\n",
    "        else:\n",
    "            da = (np.sum(self.y-self.output))\n",
    "            daz = derivative(self.architecture[self.L]['activation'], self.layers[f'a{self.L}'])\n",
    "            dZ = da*daz\n",
    "        dW = np.dot(dZ, self.layers[f'a{self.L-1}'].T)/self.noOfSamples\n",
    "        db = np.sum(dZ, axis=1, keepdims=True)/self.noOfSamples\n",
    "        dAPrev = np.dot(self.parameters[f'w{self.L}'].T, dZ)\n",
    "        derivatives[f'dW{self.L}'] = dW\n",
    "        derivatives[f'db{self.L}'] = db\n",
    "\n",
    "        for l in range(self.L-1, 0, -1):\n",
    "            dZ = dAPrev * derivative(self.architecture[l]['activation'], self.layers[f'z{l}'])\n",
    "            dW = (1./self.noOfSamples) * (np.dot(dZ, self.layers[f'a{l-1}'].T))\n",
    "            db = (1./self.noOfSamples) * (np.sum(dZ, axis=1, keepdims=True))\n",
    "            if l>1:\n",
    "                dAPrev = np.dot(self.parameters[f'w{l}'].T, dZ)\n",
    "            derivatives[f'dW{l}'] = dW\n",
    "            derivatives[f'db{l}'] = db\n",
    "\n",
    "        self.derivatives = derivatives\n",
    "\n",
    "    def fit(self, lr=0.01, epochs=1000):\n",
    "        self.costs = []\n",
    "        self.initializeParamters()\n",
    "        self.accuracies = {'train': [], 'val':[]}\n",
    "        for epoch in tqdm(range(epochs), colour='BLUE'):\n",
    "            self.forwardPropogation()\n",
    "            self.costs.append(self.cost)\n",
    "            self.backPropogation()\n",
    "            for layer in range(1, self.L+1):\n",
    "                #print(f\"Updating the parameters at layer: {layer}\")\n",
    "                self.parameters[f\"w{layer}\"] = self.parameters[\"w\"+str(layer)] - lr * self.derivatives[\"dW\" + str(layer)]\n",
    "                self.parameters[\"b\"+str(layer)] = self.parameters[\"b\"+str(layer)] - lr * self.derivatives[\"db\" + str(layer)]  \n",
    "\n",
    "            train_accuracy = self.accuracy(self.X, self.y)\n",
    "            self.accuracies[\"train\"].append(train_accuracy)\n",
    "            if self.validation is not None:\n",
    "                val_accuracy = self.accuracy(self.X_val, self.y_val)\n",
    "                self.accuracies[\"val\"].append(val_accuracy)\n",
    "\n",
    "            if epoch %10 ==0:\n",
    "                if self.validation:\n",
    "                    print(f\"Epoch: {epoch:3d} | Cost: {self.cost:.3f} | Train Accuracy: {train_accuracy:.3f} | Vaidation Accuracy: {val_accuracy:.3f}\")\n",
    "                else:\n",
    "                    print(f\"Epoch: {epoch:3d} | Cost: {self.cost:.3f} | Accuracy: {train_accuracy:.3f}\")\n",
    "\n",
    "    def accuracy(self, X, y):\n",
    "        P = self.predict(X)\n",
    "        self.labels =  np.argmax(y, axis=0)\n",
    "        self.predictions = P\n",
    "        return sum(np.equal(P, np.argmax(y, axis=0))) / y.shape[1]*100\n",
    "    \n",
    "    def predict(self, X):\n",
    "        params = self.parameters\n",
    "        values = [X]\n",
    "        for l in range(1, self.L+1):\n",
    "            z = np.dot(params[f\"w{l}\"], values[l-1]) + params[f'b{l}']\n",
    "            a = activationFunctions(z, self.architecture[l]['activation']).result\n",
    "            values.append(a)\n",
    "        if X.shape[1]>1:\n",
    "            ans = np.argmax(a, axis=0)\n",
    "        else:\n",
    "            ans = np.argmax(a)\n",
    "        \n",
    "        return ans\n",
    "\n",
    "    def plotCounts(self):\n",
    "        counts = np.unique(np.argmax(self.output, axis=0), return_conts = True)\n",
    "        plt.bar(counts[0], counts[1], color=\"navy\")\n",
    "        plt.ylabel(\"Counts\")\n",
    "        plt.xlabel(\"y_hat\")\n",
    "        plt.title(\"Distribution of Predictions\")\n",
    "        plt.show()\n",
    "\n",
    "    def plotCost(self, lr):\n",
    "        plt.figure(figsize=(8,4))\n",
    "        plt.plot(np.arange(0, len(self.costs)), self.costs, lw=1, color=\"orange\")\n",
    "        plt.title(f\"Learning rate: {lr} \\n Final Cost: {self.costs[-1]:.5f}\", fontdict = {\"family\":\"sans-serif\", \"size\":\"12\"})\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Cost\")\n",
    "        plt.show()\n",
    "\n",
    "    def plotAccuracies(self, lr):\n",
    "        acc = self.accuracies\n",
    "        fig = plt.figure(figsize=(6,4))\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.plot(acc[\"train\"], label=\"train\")\n",
    "        if self.validation is not None:\n",
    "            ax.plot(acc[\"test\"], label=\"test\")\n",
    "            ax.annotate(f\"Validation: {acc['test'][-1]:.2f}\", (len(acc[\"test\"])+4, acc[\"test\"][-1]-2), color=\"orange\")\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        ax.set_title(\"Accuracy\")\n",
    "        ax.annotate(f\"Train: {acc['train'][-1]:.2f}\", (len(acc[\"train\"])+4, acc[\"train\"][-1]+2), color=\"blue\")\n",
    "        ax.spines[\"right\"].set_visible(False)\n",
    "        ax.spines[\"top\"].set_visible(False)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 60000) (784, 10000)\n",
      "(60000,) (10000,)\n",
      "Initializing the parameters for the layer 1...\n",
      "Initializing the parameters for the layer 2...\n",
      "Initializing the parameters for the layer 3...\n",
      "Pramaters and Lengths: [('w1', (512, 784)), ('b1', (512, 1)), ('w2', (300, 512)), ('b2', (300, 1)), ('w3', (10, 300)), ('b3', (10, 1))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|\u001b[34m          \u001b[0m| 1/100 [00:17<29:02, 17.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   0 | Cost: 138410.686 | Train Accuracy: 9.930 | Vaidation Accuracy: 10.320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|\u001b[34m█         \u001b[0m| 11/100 [03:12<25:51, 17.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  10 | Cost: 138069.833 | Train Accuracy: 11.237 | Vaidation Accuracy: 11.350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|\u001b[34m██        \u001b[0m| 21/100 [06:12<24:13, 18.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  20 | Cost: 138067.496 | Train Accuracy: 11.237 | Vaidation Accuracy: 11.350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|\u001b[34m██▍       \u001b[0m| 24/100 [07:22<23:19, 18.42s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-edb4a4ff5ce9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.03\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplotCost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-61-5d4da2c7b470>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, lr, epochs)\u001b[0m\n\u001b[0;32m     95\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"b\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"b\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mderivatives\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"db\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m             \u001b[0mtrain_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracies\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"train\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_accuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidation\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-61-5d4da2c7b470>\u001b[0m in \u001b[0;36maccuracy\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m         \u001b[0mP\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mP\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-61-5d4da2c7b470>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m             \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34mf\"w{l}\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34mf'b{l}'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m             \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactivationFunctions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marchitecture\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'activation'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m             \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-1680778c2fec>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input, activation)\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselectActivation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mselectActivation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-1680778c2fec>\u001b[0m in \u001b[0;36mselectActivation\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[1;34m'tanh'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtanH\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[1;34m'relu'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m             \u001b[1;34m'leakyRelu'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleakyRelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m             \u001b[1;34m'softmax'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         }\n",
      "\u001b[1;32m<ipython-input-3-1680778c2fec>\u001b[0m in \u001b[0;36mleakyRelu\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mleakyRelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mwhere\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_test_split_no = 60000\n",
    "X_train = data.values[:train_test_split_no].T\n",
    "y_train = labels[:train_test_split_no].values.astype(int)\n",
    "X_test = data.values[train_test_split_no:].T\n",
    "y_test = labels[train_test_split_no:].values.astype(int)\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)\n",
    "\n",
    "nn = createNN(X_train, y_train, config=config, classification=True, validation=[X_test, y_test])\n",
    "epochs = 100\n",
    "lr = 0.03\n",
    "\n",
    "nn.fit(lr=lr, epochs=epochs)\n",
    "nn.plotCost(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('DL_TF')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "948d505e6a6d7f649fbd0db86d942010ad20f6083c1676336ac410c28aeae8e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
