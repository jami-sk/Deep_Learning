{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Gradients\n",
    "- Autoamtc differentiation is useful for implementing ML and DL algorithms where Backpropagation plays important role in training the algorithms\n",
    "- To differentiate automatically, TensorFlow needs to remember what operations happen in what order during the forward pass. \n",
    "- Then, during the backward pass, TensorFlow traverses this list of operations in reverse order to compute gradients."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Tapes\n",
    "- TensorFlow provides the tf.GradientTape API for automatic differentiation; that is, computing the gradient of a computation with respect to some inputs, usually tf.Variables. \n",
    "- TensorFlow \"records\" relevant operations executed inside the context of a tf.GradientTape onto a \"tape\". \n",
    "- TensorFlow then uses that tape to compute the gradients of a \"recorded\" computation using reverse mode differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Example of Derivatives wrt to a scalar\n",
    "x = tf.Variable(3.0)\n",
    "with tf.GradientTape() as tape:\n",
    "    y = x**2\n",
    "\n",
    "# Once you've recorded some operations, use GradientTape.gradient(target, sources) to calculate the gradient of some target (often a loss) relative to some source (often the model's variables):\n",
    "dy_dx = tape.gradient(y,x)\n",
    "dy_dx.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2) (3, 2)\n"
     ]
    }
   ],
   "source": [
    "### Example of Derivaties wrt a Vector\n",
    "w = tf.Variable(tf.random.normal((3,2)), name='w')\n",
    "b = tf.Variable(tf.zeros(2, dtype=tf.float32), name='b')\n",
    "x = [[1.,2.,3.]]\n",
    "with tf.GradientTape() as tape:\n",
    "    y = x@w+b\n",
    "    loss = tf.reduce_mean(y**2)\n",
    "\n",
    "# To get the gradient of loss with respect to both variables, you can pass both as sources to the gradient method. \n",
    "# The tape is flexible about how sources are passed and will accept any nested combination of lists or dictionaries and return the gradient structured the same way\n",
    "[dl_dw, dl_db] = tape.gradient(loss, [w,b])\n",
    "with tf.GradientTape() as tape:\n",
    "    y = x@w+b\n",
    "    loss = tf.reduce_mean(y**2)\n",
    "grad = tape.gradient(loss,{'w':w,'b':b})\n",
    "print(dl_dw.shape, grad['w'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense/kernel:0, shape: (3, 2)\n",
      "dense/bias:0, shape: (2,)\n"
     ]
    }
   ],
   "source": [
    "# It's common to collect tf.Variables into a tf.Module or one of its subclasses (layers.Layer, keras.Model) for checkpointing and exporting.\n",
    "# In most cases, you will want to calculate gradients with respect to a model's trainable variables. \n",
    "# Since all subclasses of tf.Module aggregate their variables in the Module.trainable_variables property, you can calculate these gradients in a few lines of code:\n",
    "layer = tf.keras.layers.Dense(2, activation='relu')\n",
    "x = tf.constant([[1., 2., 3.]])\n",
    "with tf.GradientTape() as tape:\n",
    "    y = layer(x)\n",
    "    loss = tf.reduce_mean(y**2)\n",
    "grad = tape.gradient(loss, layer.trainable_variables)\n",
    "\n",
    "for var, g in zip(layer.trainable_variables, grad):\n",
    "    print(f'{var.name}, shape: {g.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6.0, shape=(), dtype=float32)\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['x0:0']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The default behavior is to record all operations after accessing a trainable tf.Variable. The reasons for this are:\n",
    "\n",
    "#The tape needs to know which operations to record in the forward pass to calculate the gradients in the backwards pass.\n",
    "#The tape holds references to intermediate outputs, so you don't want to record unnecessary operations.\n",
    "#The most common use case involves calculating the gradient of a loss with respect to all a model's trainable variables.\n",
    "#For example, the following fails to calculate a gradient because the tf.Tensor is not \"watched\" by default, and the tf.Variable is not trainable:\n",
    "x0 = tf.Variable(3.0, name='x0')\n",
    "x1 = tf.Variable(3.0, name='x1', trainable=False) # Not trainable\n",
    "x2 = tf.Variable(2.0, name='x2') +1.0 # Returns a tensor not a variable\n",
    "x3 = tf.constant(3.0, name='x3') # not a variable\n",
    "x4 = tf.Variable(4.0, name='x4')\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y = (x0**2) + (x1**2) + (x2**2) + (x3**2)\n",
    "\n",
    "grad = tape.gradient(y,[x0,x1,x2,x3,x4])\n",
    "for g in grad:\n",
    "    print(g)\n",
    "\n",
    "# We can list the variables being watched by tape\n",
    "# tf.GradientTape provides hooks that give the user control over what is or is not watched.\n",
    "[var.name for var in tape.watched_variables()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n",
      "dys/dx0: None\n",
      "dys/dx1: 0.9999546\n"
     ]
    }
   ],
   "source": [
    "# To record gradients with respect to a tf.Tensor, you need to call GradientTape.watch(x):\n",
    "x = tf.constant(3.0)\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    y = x**2\n",
    "\n",
    "dy_dx = tape.gradient(y,x)\n",
    "print(dy_dx.numpy())\n",
    "\n",
    "# Conversely, to disable the default behavior of watching all tf.Variables, set watch_accessed_variables=False when creating the gradient tape. \n",
    "# This calculation uses two variables, but only connects the gradient for one of the variables:\n",
    "x0 = tf.Variable(0.0)\n",
    "x1 = tf.Variable(10.0)\n",
    "\n",
    "with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "  tape.watch(x1)\n",
    "  y0 = tf.math.sin(x0)\n",
    "  y1 = tf.nn.softplus(x1)\n",
    "  y = y0 + y1\n",
    "  ys = tf.reduce_sum(y)\n",
    "\n",
    "grad = tape.gradient(ys,{'x0':x0,'x1':x1})\n",
    "print('dys/dx0:', grad['x0'])\n",
    "print('dys/dx1:', grad['x1'].numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_PT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f0f48898c4b18233cc76a2987cd88405e29ffeb628d5e439f4035f1d0e31c47a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
