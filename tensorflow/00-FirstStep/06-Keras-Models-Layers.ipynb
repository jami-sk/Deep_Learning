{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "import datetime\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Models and Layers\n",
    "- In this Notebook we will examine how keras uses tf.Module.\n",
    "- tf.keras.layers.Layer is the base class of all keras layers and it inherits from tf.Moudle\n",
    "- We can convert a module into a Keras layer just by swapping out the parent and then changing __call__ to call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDense(keras.layers.Layer):\n",
    "    # adding **kwargs to support base keras layer arguments\n",
    "    def __init__(self, in_features, out_features, **kwargs):\n",
    "        super(MyDense, self).__init__(**kwargs)\n",
    "        # This will soon move to build step:\n",
    "        self.w = tf.Variable(tf.random.normal([in_features, out_features]), name='w')\n",
    "        self.b = tf.Variable(tf.zeros([out_features]), name='b')\n",
    "\n",
    "    def call(self, x):\n",
    "        y = tf.matmul(x, self.w)+self.b\n",
    "        return tf.nn.relu(y)\n",
    "simple_layer = MyDense(name=\"Simple\", in_features=3, out_features=3)\n",
    "# Keras layers have their own __call__ that does some bookkeeping described in the next section and then calls call(). You should notice no change in functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[0.       , 0.       , 1.0608517]], dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_layer([[2.0,2.0,2.0]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Build Step\n",
    "- As noted it's convenient in many cases to wait to create variables until you are sure of the input shape\n",
    "- Keras layers come with extra lifecycle step that allows you more flexibility in how you define your layers.\n",
    "- This is defined in the build function\n",
    "- build is called exactly once, and it is called with the shape of the input. it's usually used to create variables (weights)\n",
    "- We can rewrite MyDense layer above to be flexible to the size of its inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FlexibleDense(keras.layers.Layer):\n",
    "    # Note the added `**kwargs`, as Keras supports many arguments\n",
    "    def __init__(self, out_features, **kwargs):\n",
    "        super(FlexibleDense, self).__init__(**kwargs)\n",
    "        self.out_features = out_features\n",
    "    def build(self, input_shape):\n",
    "        self.w = tf.Variable(tf.random.normal([input_shape[-1], self.out_features]), name='w')\n",
    "        self.b = tf.Variable(tf.zeros([self.out_features]), name='b')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "    \n",
    "# Create an instance of the layer\n",
    "flexible_dense = FlexibleDense(out_features=3)\n",
    "#At this point, the model has not been built, so there are no variables:\n",
    "flexible_dense.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model results: tf.Tensor(\n",
      "[[-0.31626916  3.6292317   0.11567521]\n",
      " [-0.4744041   5.443848    0.17351276]], shape=(2, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Calling the function allocates appropriately-sized variables:\n",
    "print(\"Model results:\", flexible_dense(tf.constant([[2.0, 2.0, 2.0], [3.0, 3.0, 3.0]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'flexible_dense/w:0' shape=(3, 3) dtype=float32, numpy=\n",
       " array([[-1.5037673 ,  1.0501281 , -0.6804019 ],\n",
       "        [ 0.2773653 ,  0.8656277 ,  0.5130308 ],\n",
       "        [ 1.0682673 , -0.10113989,  0.2252087 ]], dtype=float32)>,\n",
       " <tf.Variable 'flexible_dense/b:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flexible_dense.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed: Exception encountered when calling layer \"flexible_dense\" (type FlexibleDense).\n",
      "\n",
      "Matrix size-incompatible: In[0]: [1,4], In[1]: [3,3] [Op:MatMul]\n",
      "\n",
      "Call arguments received by layer \"flexible_dense\" (type FlexibleDense):\n",
      "  â€¢ inputs=tf.Tensor(shape=(1, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Since build is only called once, inputs will be rejected if the input shape is not compatible with the layer's variables:\n",
    "try:\n",
    "  print(\"Model results:\", flexible_dense(tf.constant([[2.0, 2.0, 2.0, 2.0]])))\n",
    "except tf.errors.InvalidArgumentError as e:\n",
    "  print(\"Failed:\", e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras layers have a lot more extra features including:  \n",
    "    - Optional losses  \n",
    "    - Support for metrics  \n",
    "    - Built-in support for an optional training argument to differentiate between training and inference use  \n",
    "    - get_config and from_config methods that allow you to accurately store configurations to allow model cloning in Python  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Models\n",
    "- We can define your model as nested Keras layers.\n",
    "- However, Keras also provides a full-featured model class called tf.keras.Model. \n",
    "- It inherits from tf.keras.layers.Layer, so a Keras model can be used, nested, and saved in the same way as Keras layers. \n",
    "- Keras models come with extra functionality that makes them easy to train, evaluate, load, save, and even train on multiple machines.\n",
    "- We can define the SequentialModule from above with nearly identical code, again converting __call__ to call() and changing the parent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model results: tf.Tensor([[-10.384374   -1.9303488]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "class MySequentialModel(tf.keras.Model):\n",
    "  def __init__(self, name=None, **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "\n",
    "    self.dense_1 = FlexibleDense(out_features=3)\n",
    "    self.dense_2 = FlexibleDense(out_features=2)\n",
    "  def call(self, x):\n",
    "    x = self.dense_1(x)\n",
    "    return self.dense_2(x)\n",
    "\n",
    "# You have made a Keras model!\n",
    "my_sequential_model = MySequentialModel(name=\"the_model\")\n",
    "\n",
    "# Call it on a tensor, with random results\n",
    "print(\"Model results:\", my_sequential_model(tf.constant([[2.0, 2.0, 2.0]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'my_sequential_model/flexible_dense_1/w:0' shape=(3, 3) dtype=float32, numpy=\n",
       " array([[ 0.6732569 , -0.74500614, -1.0023254 ],\n",
       "        [-0.72728425,  0.5621365 , -0.8005991 ],\n",
       "        [ 1.6960504 ,  0.11627781, -0.75572777]], dtype=float32)>,\n",
       " <tf.Variable 'my_sequential_model/flexible_dense_1/b:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'my_sequential_model/flexible_dense_2/w:0' shape=(3, 2) dtype=float32, numpy=\n",
       " array([[-1.3194591 ,  0.35605294],\n",
       "        [-1.1395929 ,  0.7418711 ],\n",
       "        [ 1.2121584 ,  0.5864098 ]], dtype=float32)>,\n",
       " <tf.Variable 'my_sequential_model/flexible_dense_2/b:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All the same features are available, including tracking variables and submodules.\n",
    "# Note: To emphasize the note above, a raw tf.Module nested inside a Keras layer or model will not get its variables collected for training or saving. \n",
    "# Instead, nest Keras layers inside of Keras layers.\n",
    "my_sequential_model.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<__main__.FlexibleDense at 0x23b0f9f4f10>,\n",
       " <__main__.FlexibleDense at 0x23b0f722ac0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_sequential_model.submodules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 3)]               0         \n",
      "                                                                 \n",
      " flexible_dense_3 (FlexibleD  (None, 3)                12        \n",
      " ense)                                                           \n",
      "                                                                 \n",
      " flexible_dense_4 (FlexibleD  (None, 2)                8         \n",
      " ense)                                                           \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20\n",
      "Trainable params: 20\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Overriding tf.keras.Model is a very Pythonic approach to building TensorFlow models. If you are migrating models from other frameworks, this can be very straightforward.\n",
    "# If you are constructing models that are simple assemblages of existing layers and inputs, you can save time and space by using the functional API, \n",
    "# which comes with additional features around model reconstruction and architecture.\n",
    "# Here is the same model with the functional API:\n",
    "\n",
    "inputs = tf.keras.Input(shape=[3,])\n",
    "\n",
    "x = FlexibleDense(3)(inputs)\n",
    "x = FlexibleDense(2)(x)\n",
    "\n",
    "my_functional_model = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "\n",
    "my_functional_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[3.856375, 9.040522]], dtype=float32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The major difference here is that the input shape is specified up front as part of the functional construction process. \n",
    "# The input_shape argument in this case does not have to be completely specified; you can leave some dimensions as None.\n",
    "# Note: You do not need to specify input_shape or an InputLayer in a subclassed model; these arguments and layers will be ignored.\n",
    "my_functional_model(tf.constant([[2.0, 2.0, 2.0]]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Keras Models\n",
    "- Keras models can be checkpointed, and that will look the same as tf.Module.\n",
    "- Keras models can also be saved with tf.saved_model.save(), as they are modules. \n",
    "- However, Keras models have convenience methods and other functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: exname_of_file\\assets\n"
     ]
    }
   ],
   "source": [
    "my_sequential_model.save(\"exname_of_file\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "# Just as easily, they can be loaded back in:\n",
    "reconstructed_model = tf.keras.models.load_model(\"exname_of_file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-10.384374 ,  -1.9303488]], dtype=float32)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keras SavedModels also save metric, loss, and optimizer states.\n",
    "# This reconstructed model can be used and will produce the same result when called on the same data:\n",
    "reconstructed_model(tf.constant([[2.0, 2.0, 2.0]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_PT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f0f48898c4b18233cc76a2987cd88405e29ffeb628d5e439f4035f1d0e31c47a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
